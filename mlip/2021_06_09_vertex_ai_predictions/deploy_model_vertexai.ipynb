{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook will guide you through how you can deploy a Tensorflow model on [Vertex AI](https://cloud.google.com/vertex-ai). To deploy XGBoost, scikit-learn or other models, please refer to the Vertex AI documentation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Install necessary python packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install -U google-cloud-aiplatform"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip3 install google-cloud-storage"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Prerequisites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* You have a GCP project set up\n",
    "* A Tensorflow model is saved on a GCS bucket"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Parameter settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROJECT_ID = \"[your-project-id]\"  # @param {type:\"string\"}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if PROJECT_ID == \"\" or PROJECT_ID is None or PROJECT_ID == \"[your-project-id]\":\n",
    "    # Get your GCP project id from gcloud\n",
    "    shell_output = !gcloud config list --format 'value(core.project)' 2>/dev/null\n",
    "    PROJECT_ID = shell_output[0]\n",
    "    print(\"Project ID:\", PROJECT_ID)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "REGION = \"europe-west1\"  # @param {type: \"string\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Name for the model that we'll deploy. Will be the name given to the model model and endpoint that will be created in this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: uncomment and fill in\n",
    "# MODEL_NAME=\"<MODEL_NAME>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Models to be served on an endpoint need to first be created on [Vertex AI Models](https://console.cloud.google.com/vertex-ai/models). They can be created from a training or by importing from\n",
    "your model artifacts. In the case of a Tensorflow model, this is the folder containing your variables and .pb file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: uncomment and fill in\n",
    "# ARTIFACT_LOCATION_GCS = \"<GCS_PATH>\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Machine type for serving. If required, GPUs can be added. See [guidelines](https://cloud.google.com/vertex-ai/docs/predictions/configure-compute#gpus) which GPUs are supported by the machines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SERVING_MACHINE_TYPE=\"n1-standard-2\"\n",
    "SERVING_GPU, SERVING_NGPU = (None, None)  # example: (aip.gapic.AcceleratorType.NVIDIA_TESLA_K80.name, 2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Serving from Vertex AI endpoints is done from Docker images that run an HTTP server. The image is selected here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TF = \"2-4\"  # 1.15 to 2.4 is supported at the time of writing (26/05/2021) \n",
    "if TF[0] == \"2\":\n",
    "    if SERVING_GPU:\n",
    "        SERVING_VERSION = \"tf2-gpu.{}\".format(TF)\n",
    "    else:\n",
    "        SERVING_VERSION = \"tf2-cpu.{}\".format(TF)\n",
    "else:\n",
    "    if SERVING_GPU:\n",
    "        SERVING_VERSION = \"tf-gpu.{}\".format(TF)\n",
    "    else:\n",
    "        SERVING_VERSION = \"tf-cpu.{}\".format(TF)\n",
    "\n",
    "SERVING_IMAGE_URI = \"europe-docker.pkg.dev/vertex-ai/prediction/{}:latest\".format(SERVING_VERSION)\n",
    "\n",
    "print(\"Deployment:\", SERVING_IMAGE_URI, SERVING_GPU, SERVING_NGPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from google.cloud import aiplatform as aip\n",
    "from typing import Optional, Sequence, Dict, Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# API service endpoint\n",
    "API_ENDPOINT = \"{}-aiplatform.googleapis.com\".format(REGION)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/model-builder/upload_model_sample.py\n",
    "def upload_model_sample(\n",
    "    project: str,\n",
    "    location: str,\n",
    "    display_name: str,\n",
    "    serving_container_image_uri: str,\n",
    "    artifact_uri: Optional[str] = None,\n",
    "    serving_container_predict_route: Optional[str] = None,\n",
    "    serving_container_health_route: Optional[str] = None,\n",
    "    description: Optional[str] = None,\n",
    "    serving_container_command: Optional[Sequence[str]] = None,\n",
    "    serving_container_args: Optional[Sequence[str]] = None,\n",
    "    serving_container_environment_variables: Optional[Dict[str, str]] = None,\n",
    "    serving_container_ports: Optional[Sequence[int]] = None,\n",
    "    instance_schema_uri: Optional[str] = None,\n",
    "    parameters_schema_uri: Optional[str] = None,\n",
    "    prediction_schema_uri: Optional[str] = None,\n",
    "    explanation_metadata: Optional[aip.explain.ExplanationMetadata] = None,\n",
    "    explanation_parameters: Optional[aip.explain.ExplanationParameters] = None,\n",
    "    sync: bool = True):\n",
    "    \"\"\"Function to upload a model to Vertex AI Models.\n",
    "    \n",
    "    Args:\n",
    "        project (str): Required. Project ID.\n",
    "        location (str): Required. Region you want to upload the model to.\n",
    "        display_name (str): Required. The display name of the Model. The name can be up to 128\n",
    "        characters long and can be consist of any UTF-8 characters.\n",
    "        serving_container_image_uri (str): Required. The URI of the Model serving container.\n",
    "    Information about the remaining parameters can be found here:\n",
    "    https://github.com/googleapis/python-aiplatform/blob/master/google/cloud/aiplatform/models.py\n",
    "    \"\"\"\n",
    "\n",
    "    aip.init(project=project, location=location)\n",
    "\n",
    "    model = aip.Model.upload(\n",
    "        display_name=display_name,\n",
    "        artifact_uri=artifact_uri,\n",
    "        serving_container_image_uri=serving_container_image_uri,\n",
    "        serving_container_predict_route=serving_container_predict_route,\n",
    "        serving_container_health_route=serving_container_health_route,\n",
    "        instance_schema_uri=instance_schema_uri,\n",
    "        parameters_schema_uri=parameters_schema_uri,\n",
    "        prediction_schema_uri=prediction_schema_uri,\n",
    "        description=description,\n",
    "        serving_container_command=serving_container_command,\n",
    "        serving_container_args=serving_container_args,\n",
    "        serving_container_environment_variables=serving_container_environment_variables,\n",
    "        serving_container_ports=serving_container_ports,\n",
    "        explanation_metadata=explanation_metadata,\n",
    "        explanation_parameters=explanation_parameters,\n",
    "        sync=sync,\n",
    "    )\n",
    "\n",
    "    model.wait()\n",
    "\n",
    "    print(model.display_name)\n",
    "    print(model.resource_name)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Upload the model. Since we're importing a model from a GCS bucket, we\n",
    "need to specify the artifact uri.\n",
    "\n",
    "This would not have to be required  if we were to have created a model with custom training. This would require a custom container as explained [here](https://cloud.google.com/vertex-ai/docs/predictions/use-custom-container#aiplatform_upload_model_highlight_container-python). The parameters starting with `serving_container_*` will then need to be set accordingly.\n",
    "\n",
    "This would also not have been required if we were to have created an AutoML model. As can be seen [here](https://cloud.google.com/vertex-ai/docs/training/automl-api#aiplatform_create_training_pipeline_image_classification_sample-python), an aiplatform.AutoMLImageTrainingJob.run() creates an\n",
    "`aiplatform.models.Model` that can call the `.upload()` function without needing to specify the artifact uri."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = upload_model_sample(\n",
    "    project=PROJECT_ID,\n",
    "    location=REGION,\n",
    "    display_name=f\"{MODEL_NAME}_{TIMESTAMP}\",\n",
    "    serving_container_image_uri = SERVING_IMAGE_URI,\n",
    "    artifact_uri=ARTIFACT_LOCATION_GCS\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Enpoints are easy to create and have no models assigned to them initially. We can consider them as placeholders. The endpoint ID, necessary for calling a prediction, is now fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/model-builder/create_endpoint_sample.py\n",
    "def create_endpoint_sample(\n",
    "    project: str,\n",
    "    display_name: str,\n",
    "    location: str):\n",
    "    \"\"\"Function to create an endpoint on Vertex AI Endpoints.\n",
    "    \n",
    "    Args:\n",
    "        project (str): Required. Project ID.\n",
    "        location (str): Required. Region to retreive an endpoint from.\n",
    "        display_name (str): Required. The display name of the Model. The name can be up to 128\n",
    "        characters long and can be consist of any UTF-8 characters.  \n",
    "    \"\"\"\n",
    "    aip.init(project=project, location=location)\n",
    "\n",
    "    endpoint = aip.Endpoint.create(\n",
    "        display_name=display_name, project=project, location=location,\n",
    "    )\n",
    "\n",
    "    print(endpoint.display_name)\n",
    "    print(endpoint.resource_name)\n",
    "    return endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint = create_endpoint_sample(\n",
    "    project=PROJECT_ID,\n",
    "    display_name=f\"{MODEL_NAME}_{TIMESTAMP}\",\n",
    "    location=REGION)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Endpoint ID:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### No models have been deployed on the endpoint yet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.list_models()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deploy model on endpoint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# From https://github.com/googleapis/python-aiplatform/blob/HEAD/samples/model-builder/deploy_model_with_dedicated_resources_sample.py\n",
    "def deploy_model_with_dedicated_resources_sample(\n",
    "    project,\n",
    "    location,\n",
    "    model_name: str,\n",
    "    machine_type: str,\n",
    "    endpoint: Optional[aip.Endpoint] = None,\n",
    "    deployed_model_display_name: Optional[str] = None,\n",
    "    traffic_percentage: Optional[int] = 0,\n",
    "    traffic_split: Optional[Dict[str, int]] = None,\n",
    "    min_replica_count: int = 1,\n",
    "    max_replica_count: int = 1,\n",
    "    accelerator_type: Optional[str] = None,\n",
    "    accelerator_count: Optional[int] = None,\n",
    "    explanation_metadata: Optional[aip.explain.ExplanationMetadata] = None,\n",
    "    explanation_parameters: Optional[aip.explain.ExplanationParameters] = None,\n",
    "    metadata: Optional[Sequence[Tuple[str, str]]] = (),\n",
    "    sync: bool = True,\n",
    "):\n",
    "    \"\"\"Function to deploy a model on an endpoint.\n",
    "    \n",
    "    Args:\n",
    "        project (str): Required. Project ID.\n",
    "        location (str): Required. Region endpoint is set up.\n",
    "        display_name (str): Required. The display name of the Model. The name can be up to 128\n",
    "        characters long and can be consist of any UTF-8 characters.\n",
    "        model_name (str): Required. ID of model.\n",
    "    Information about the remaining parameters can be found here:\n",
    "    https://github.com/googleapis/python-aiplatform/blob/master/google/cloud/aiplatform/models.py\n",
    "    \"\"\"\n",
    "\n",
    "    aip.init(project=project, location=location)\n",
    "\n",
    "    model = aip.Model(model_name=model_name)\n",
    "\n",
    "    # The explanation_metadata and explanation_parameters should only be\n",
    "    # provided for a custom trained model and not an AutoML model.\n",
    "    model.deploy(\n",
    "        endpoint=endpoint,\n",
    "        deployed_model_display_name=deployed_model_display_name,\n",
    "        traffic_percentage=traffic_percentage,\n",
    "        traffic_split=traffic_split,\n",
    "        machine_type=machine_type,\n",
    "        min_replica_count=min_replica_count,\n",
    "        max_replica_count=max_replica_count,\n",
    "        accelerator_type=accelerator_type,\n",
    "        accelerator_count=accelerator_count,\n",
    "        explanation_metadata=explanation_metadata,\n",
    "        explanation_parameters=explanation_parameters,\n",
    "        metadata=metadata,\n",
    "        sync=sync,\n",
    "    )\n",
    "\n",
    "    model.wait()\n",
    "\n",
    "    print(model.display_name)\n",
    "    print(model.resource_name)\n",
    "    return model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some important metrics for deployment:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "min_replica_count: The minimum number of nodes for this deployment. The node count can be increased or decreased as required by the prediction load, up to the maximum number of nodes, but will never fall below this number.\n",
    "\n",
    "max_replica_count: The maximum number of nodes for this deployment. The node count can be increased or decreased as required by the prediction load, but will never exceed the maximum. If you omit the max_replica_count parameter, then maximum number of nodes is set to the value of min_replica_count.\n",
    "\n",
    "traffic_percentage (int): Optional. Desired traffic to newly deployed model. Defaults to 0 if there are pre-existing deployed models. Defaults to 100 if there are no pre-existing deployed models. Negative values should not be provided. Traffic of previously deployed models at the endpoint will be scaled down to accommodate new deployed model's traffic. Should not be provided if traffic_split is provided.\n",
    "\n",
    "traffic_split (Dict[str, int]): Optional. A map from a DeployedModel's ID to the percentage of this Endpoint's traffic that should be forwarded to that DeployedModel. If a DeployedModel's ID is not listed in this map, then it receives no traffic. The traffic percentage values must add up to 100, or map must be empty if the Endpoint is to not accept any traffic at the moment. Key for model being deployed is \"0\". Should not be provided if traffic_percentage is provided.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "deploy_model_with_dedicated_resources_sample(\n",
    "    location=REGION,\n",
    "    project=PROJECT_ID,\n",
    "    endpoint=endpoint,\n",
    "    model_name=model.name,\n",
    "    deployed_model_display_name=model.display_name,\n",
    "    machine_type=SERVING_MACHINE_TYPE,\n",
    "    accelerator_type=SERVING_GPU,\n",
    "    accelerator_count=SERVING_NGPU\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint.deploy_models()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
