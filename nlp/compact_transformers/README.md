# Compact Transformers

We all know bigger is often better! Nothing cooler than rolling up in your larger-than-life Transformer model. Who doesn't want to drive a monster truck once in their life...

However, monster trucks aren't always practical in real life. Models need to run smoothly in production. So the best model size is often the one most suited for your practical situation and requirements.

In this tip, we'll have a look at some sub-25million parameter transformer models available in Huggingface. So you can have both a smoothly running application and still brag to your manager that you implemented transformers!

We recommend to open the notebook using Colab, to check how you can finetune these baby-BERTs for a simple text classification task ðŸ‘‡:

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ml6team/quick-tips/blob/main/nlp/2021_02_26_compact_transformers/compact_transformers.ipynb)