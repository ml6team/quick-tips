# ML6 NLP Quick Tips

Current content:

-  [_Multilingual Sentence Embeddings_](multilingual_sentence_embeddings):
Gives an overview of various current multilingual sentence embedding techniques and tools, and
how they compare given various sequence lengths.

-  [_Spacy 3.0_](spacy_3_projects):
Spacy 3.0 has just been released and in this tip, we'll have a look at some of the new features.
We'll be training a German NER model and streamline the end-to-end pipeline using the brand new spaCy projects!

-  [_Compact transformers_](compact_transformers):
Bigger isn't always better. In this tip we look at some compact BERT-based models that provide a nice balance
between computational resources and model accuracy.

-  [_Keyword Extraction with pke_](pke_keyword_extraction):
The KEYNG (read *king*) is dead, long live the KEYNG!
In this tip we look at `pke`, an alternative to Gensim for keyword extraction.

-  [_Explainable transformers using SHAP_](shap_for_huggingface_transformers):
BERT, explain yourself! üìñ
Up until recently language model predictions have lacked transparency. In this tip we look at `SHAP`, a way to explain your latest transformer based models.

-  [_Transformer-based Data Augmentation_](data_augmentation):
Ever struggled with having a limited non-English NLP dataset for a project? Fear not, data augmentation to the rescue ‚õëÔ∏è
In this week's tip, we look at backtranslation üîÄ and contextual word embedding insertions as data augmentation techniques for multilingual NLP. 

-  [_Long range transformers_](long_range_transformers):
Beyond and above the 512! üèÖ In this week's tip, we look at novel long range transformer architectures and compare them against the well-known RoBERTa model.

-  [_Neural Keyword Extraction_](neural_keyword_extraction):
Neural Keyword Extraction üß†
In this week's tip, we look at neural keyword extraction methods and how they compare to classical methods.

-  [_HuggingFace Optimum_](huggingface_optimum):
HuggingFace Optimum Quantization ‚úÇÔ∏è
In this week's tip, we take a look at the new HuggingFace Optimum package to check out some model quantization techniques.

- [ _Text Augmentation using large-scale LMs and prompt engineering_](augmentation_lm):
Typically, the more data we have, the better performance we can achieve ü§ô. However, it is sometimes difficult and/or expensive to annotate a large amount of training data üòû. In this tip, we leverage three large-scale LMs (GPT-3, GPT-J and GPT-Neo) to generate very realistic samples from a very small dataset.
