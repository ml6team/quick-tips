# Transformer-based Data Augmentation

Ever struggled with having a limited non-English NLP dataset for a project? ðŸ¤¯ Fear not, data augmentation to the rescue â›‘
In this week's tip, we look at backtranslation ðŸ”€ and contextual word embedding insertions as data augmentation techniques for multilingual NLP. We'll be using the MariaMT and distilled BERT pre-trained models, available on huggingface. 

The training size will impact the performace of a model heavily, this notebook looks into the possibilities of performing data augmentation on a NLP dataset. Data augmentation techniques are used to generate additional samples. Data augmentation is already standard practice in computer vision projects ðŸ‘Œ, but can also be leveraged in multilingual NLP problems. 

We recommend to open the notebook using Colab for an interactive explainable experience and optimal rendering of the visuals ðŸ‘‡:

[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/ml6team/quick-tips/blob/main/nlp/2021_06_18_data_augmentation/totw_nlp_dat_aug.ipynb)
