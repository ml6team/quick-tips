{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ne2ic1nmhs0n"
   },
   "source": [
    "# Neural Keyword Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "j78lHqRwsUeU"
   },
   "source": [
    "In a [pervious quick tip](https://github.com/ml6team/quick-tips/tree/main/nlp/2021_03_18_pke_keyword_extraction) we looked at [**pke**](https://boudinfl.github.io/pke/build/html/index.html) as a replacement for Gensim's [recently removed keyword extraction module](https://github.com/RaRe-Technologies/gensim/releases/tag/4.0.0).\n",
    "pke comes with batteries included: \n",
    "\n",
    "* It has **preprocessing built-in**, \n",
    "* supports **non-English languages**, and \n",
    "* provides a **wide range of keyword extraction methods**: statistical, graph-based, and supervised.\n",
    "\n",
    "This makes pke a **great choice to get started** with keyword extraction, experiment with different methods and generate baselines to improve upon.\n",
    "\n",
    "But what if the required performance is not met by these *classical* methods?\n",
    "\n",
    "## The Future of Keyword Extraction\n",
    "\n",
    "Newly-developed auspicious extraction methods fall into the category of **neural keyword extraction**.\n",
    "\n",
    "The methods often utilise **sequence-to-sequence models** based on recurrent neural networks (RNNs), Long Short-Term Memory (LSTM), or Transformers. Their objective is to transform a sequence of input words (the given document) into an abstract intermediate representation and generate a sequence of keywords from it.\n",
    "\n",
    "These models do not use words or phrases directly, which enables them to generate unseen keywords as well. This is called **keyword generation** and combines abstractive as well as extractive keywords.\n",
    "\n",
    "They report **impressive performance** increases over the classical extraction methods. However, training such models **requires a large collection of documents** and annotated keywords, since the final training step is usually supervised.\n",
    "\n",
    "In addition, many of the models' repositories are not well maintained which makes it **difficult to train** them, especially on different languages or domains. \n",
    "\n",
    "This leaves the quesition of how neural keyword extraction can already be used today.\n",
    "\n",
    "Follow the below sections in this notebook to learn how to use **two approaches in the neural keyword extraction category** and how they compare to classical extraction methods. üëá"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ak0IG15fsUVV"
   },
   "source": [
    "## üèó Getting started: Install packages & download models\n",
    "\n",
    "The below cells will setup everything that is required to get started with keyword extraction:\n",
    "\n",
    "* Install packages\n",
    "* Download additional resources"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qbMg6Xyj8Xww",
    "outputId": "3b0b8599-77d5-41bf-b295-0ca49733b28e"
   },
   "outputs": [],
   "source": [
    "!pip install --quiet git+https://github.com/boudinfl/pke.git\n",
    "!pip install --quiet transformers\n",
    "!pip install --quiet keybert\n",
    "\n",
    "# Download additional resources\n",
    "!python -m nltk.downloader stopwords\n",
    "!python -m nltk.downloader universal_tagset\n",
    "!python -m spacy download en # Download English model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vEu54fBNsUPR"
   },
   "source": [
    "## KeyBERT\n",
    "\n",
    "Strictly speaking [KeyBERT](https://github.com/MaartenGr/KeyBERT) is not an end-to-end neural keyword extraction model. \n",
    "\n",
    "Nonetheless the **underlying idea** is as clever as it is simple: It compares embeddings of words with embeddings of texts and selects the set of keywords which are most similar to the entire text.\n",
    "Both **word embeddings** as well as **text embeddings** are generated using **state-of-the-art neural models**, which have lead to tremendous performance improvements in other tasks.\n",
    "\n",
    "The benefit of this approach is that the used models are trained in an unsupervised manner and thus **do not require an annotated dataset** of keywords. In addition, they are available in a number of **non-English languages** as well.\n",
    "\n",
    "Let's see below how KeyBert can be used in practice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "mV6d1GCs3sY6"
   },
   "outputs": [],
   "source": [
    "from keybert import KeyBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "3FdbT3Mi3xtY"
   },
   "outputs": [],
   "source": [
    "kw_model = KeyBERT()\n",
    "\n",
    "def extract_keybert_keywords(text, keyphrase_ngram_range=(1,1), stop_words='english',\n",
    "                             use_maxsum=True, nr_candidates=20, top_n=5,\n",
    "                             use_mmr=False, diversity=0.7,\n",
    "                             only_keywords=True):\n",
    "    keyphrases = kw_model.extract_keywords(text, keyphrase_ngram_range=keyphrase_ngram_range, \n",
    "                                           stop_words=stop_words, use_maxsum=use_maxsum, \n",
    "                                           use_mmr=use_mmr, diversity=diversity, \n",
    "                                           nr_candidates=nr_candidates, top_n=top_n)\n",
    "    if only_keywords:\n",
    "        keyphrases = [phrase for phrase, score in keyphrases]\n",
    "    return keyphrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dmYw5b02sUFq"
   },
   "source": [
    "## BART-based model\n",
    "\n",
    "ü§ó Hugging Face Model Hub is *the* prime address when it comes to discovering state-of-the-art models that are easy to use. However, with respect to neural keyword (or keyphrase) extraction there are only two models available as of this writing.\n",
    "\n",
    "Both models were created by [Ankur Singh](https://huggingface.co/ankur310794) and use a **BART-based sequence-to-sequence architecture**. Unfortunately there are not a lot of details available about the training specifics.\n",
    "\n",
    "* [One model](https://huggingface.co/ankur310794/bart-base-keyphrase-generation-kpTimes) was trained using the [KPTimes dataset](https://aclanthology.org/W19-8617/), a large dataset consisting of **English news articles** and a variable number of hand-annotated keywords.\n",
    "* [The other](https://huggingface.co/ankur310794/bart-base-keyphrase-generation-openkp) was trained using the [OpenKP dataset](https://github.com/microsoft/OpenKP), which contains a large number of **English web documents** and up to three most relevant keywords. This restriction holds true for the model as well: it will return at most three keywords."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "qnexjK78yvYr"
   },
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
    "  \n",
    "huggingface_models = {\n",
    "    # Trained on OpenKP: Returns up to 3 keywords\n",
    "    'openkp': \"ankur310794/bart-base-keyphrase-generation-openkp\",\n",
    "    # Trained on KPTimes\n",
    "    'kptimes': \"ankur310794/bart-base-keyphrase-generation-kpTimes\",\n",
    "}\n",
    "\n",
    "def load_model(model_name):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    return tokenizer, model\n",
    "\n",
    "tokenizer, model = load_model(huggingface_models['kptimes'])\n",
    "\n",
    "def extract_keywords_using_bart(text):\n",
    "    encoded_text = tokenizer.prepare_seq2seq_batch(\n",
    "        [text],\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "    encoded_keywords = model.generate(**encoded_text)\n",
    "    raw_keywords = tokenizer.batch_decode(\n",
    "        encoded_keywords, \n",
    "        skip_special_tokens=True,\n",
    "    )\n",
    "    keywords = [keyword.strip() for keyword_string in raw_keywords\n",
    "                                for keyword in keyword_string.split(';')]\n",
    "    return keywords"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uaCNrz678qXF"
   },
   "source": [
    "## ‚öîÔ∏è Classical vs. Neural Keyword Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ertAe4GXCal5"
   },
   "source": [
    "### Classical extraction methods\n",
    "\n",
    "The code below wraps several extraction methods from pke into convenience functions that use the default parameters and only require an (English) text from which keywords will be extracted."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "2xhjI7xo8pK5"
   },
   "outputs": [],
   "source": [
    "import string\n",
    "from itertools import zip_longest\n",
    "\n",
    "import pke\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# Convenience functions for pke keyword extraction\n",
    "\n",
    "## Statistical models\n",
    "def extract_tfidf_keywords(text, top_n=10, language='en', normalization=None, \n",
    "                           n_grams=3, only_keywords=True):\n",
    "    stoplist = list(string.punctuation)\n",
    "    stoplist += stopwords.words('english')\n",
    "    extractor = pke.unsupervised.TfIdf()\n",
    "    extractor.load_document(input=text, language=language, normalization=normalization)\n",
    "    extractor.candidate_selection(n=n_grams, stoplist=stoplist)\n",
    "    extractor.candidate_weighting()\n",
    "    keyphrases = extractor.get_n_best(n=top_n)\n",
    "    if only_keywords:\n",
    "        keyphrases = [phrase for phrase, score in keyphrases]\n",
    "    return keyphrases\n",
    "\n",
    "def extract_yake_keywords(text, top_n=10, normalization=None, window=2, \n",
    "                          threshold=0.8, language='en', n=3, use_stems=False, \n",
    "                          only_keywords=True):\n",
    "    stoplist = stopwords.words('english')\n",
    "    extractor = pke.unsupervised.YAKE()\n",
    "    extractor.load_document(input=text, language=language, normalization=normalization)\n",
    "    extractor.candidate_selection(n=n, stoplist=stoplist)\n",
    "    extractor.candidate_weighting(window=window, stoplist=stoplist, use_stems=use_stems)\n",
    "    keyphrases = extractor.get_n_best(n=top_n, threshold=threshold)\n",
    "    if only_keywords:\n",
    "        keyphrases = [phrase for phrase, score in keyphrases]\n",
    "    return keyphrases\n",
    "\n",
    "## Graph-based algorithms\n",
    "def extract_textrank_keywords(text, top_n=10, language='en', normalization=None, \n",
    "                              window=2, top_percent=0.33, only_keywords=True):\n",
    "    pos = {'NOUN', 'PROPN', 'ADJ'}\n",
    "    extractor = pke.unsupervised.TextRank()\n",
    "    extractor.load_document(input=text, language=language, normalization=normalization)\n",
    "    extractor.candidate_weighting(window=window, pos=pos, top_percent=top_percent)\n",
    "    keyphrases = extractor.get_n_best(n=top_n)\n",
    "    if only_keywords:\n",
    "        keyphrases = [phrase for phrase, score in keyphrases]\n",
    "    return keyphrases\n",
    "\n",
    "def extract_topicrank_keywords(text, top_n=10, language='en', only_keywords=True):\n",
    "    extractor = pke.unsupervised.TopicRank()\n",
    "    extractor.load_document(input=text, language=language)\n",
    "    extractor.candidate_selection()\n",
    "    extractor.candidate_weighting()\n",
    "    keyphrases = extractor.get_n_best(n=top_n)\n",
    "    if only_keywords:\n",
    "        keyphrases = [phrase for phrase, score in keyphrases]\n",
    "    return keyphrases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GI8OguDTcYL1"
   },
   "source": [
    "The next cell:\n",
    "\n",
    "* **collects the above functions** for keyword extraction together with a set of keyword arguments for easy access in a dictionary,\n",
    "* sets a **default subset of extraction functions** to compare, and \n",
    "* defines a **convenience function** that simplifies the **comparison** of the different extraction methods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "uGa7LqW2-Omy"
   },
   "outputs": [],
   "source": [
    "# Define extraction functions, labels, and set parameters\n",
    "top_n = 10\n",
    "\n",
    "KEYWORD_EXTRACTION_FUNCTIONS = {\n",
    "    # Neural Keyword Extraction \n",
    "    'KeyBERT': (\n",
    "        extract_keybert_keywords, \n",
    "        {\n",
    "            'keyphrase_ngram_range': (1,2),\n",
    "            'stop_words': 'english',\n",
    "            'use_maxsum': True, \n",
    "            'nr_candidates': 20, \n",
    "            'top_n': 10, \n",
    "            'use_mmr': False, \n",
    "            'diversity': 0.7,\n",
    "        },\n",
    "    ),\n",
    "    'BART-based': (\n",
    "        extract_keywords_using_bart, \n",
    "        {},\n",
    "    ),\n",
    "\n",
    "    # Statistical models\n",
    "    'TFIDF': (\n",
    "        extract_tfidf_keywords, \n",
    "        {'top_n': top_n},\n",
    "    ),\n",
    "    'YAKE': (\n",
    "        extract_yake_keywords, \n",
    "        {'top_n': top_n},\n",
    "    ),\n",
    "    # Graph-based models\n",
    "    'TextRank': (\n",
    "        extract_textrank_keywords, \n",
    "        {'top_n': top_n ,'window': 2},\n",
    "    ),\n",
    "    'TopicRank': (\n",
    "        extract_topicrank_keywords, \n",
    "        {'top_n': top_n},\n",
    "    ),\n",
    "}\n",
    "\n",
    "DEFAULT_SELECTION = ['KeyBERT', 'BART-based', 'TFIDF', 'YAKE', 'TextRank', 'TopicRank']\n",
    "\n",
    "def compare_keyword_extraction_algorithms(text, \n",
    "                                          keyword_extraction_functions=None,\n",
    "                                          selection=None):\n",
    "    \"\"\"Convenience function compare extracted keywords from the given text.\n",
    "\n",
    "    Args:\n",
    "        text (str): Text to extract keywords from.\n",
    "        keyword_extraction_functions (dict): Dict contaning labels as keys and\n",
    "            a tuple of (extraction_function, kwargs) as values. Defaults to None.\n",
    "        selection (list): List of names of algorithm to use for keyword \n",
    "            extraction. See keyword_extraction_functions for possible values\n",
    "            and/or to change arguments. Defaults to None.\n",
    "    \"\"\"\n",
    "    if keyword_extraction_functions is None:\n",
    "        keyword_extraction_functions = KEYWORD_EXTRACTION_FUNCTIONS\n",
    "    if selection is None:\n",
    "        selection = DEFAULT_SELECTION\n",
    "    \n",
    "    # Create DataFrame with extracted keywords\n",
    "    all_keywords = pd.DataFrame(\n",
    "        zip_longest(\n",
    "            *(extraction_fn(text, **kwargs)\n",
    "                for name, (extraction_fn, kwargs) in keyword_extraction_functions.items()\n",
    "                if name in selection\n",
    "            ),\n",
    "            fillvalue=\"\",\n",
    "        ),\n",
    "        columns=selection,\n",
    "    )\n",
    "    \n",
    "    # Display table\n",
    "    display(all_keywords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9c12HR6HKWol"
   },
   "source": [
    "### Extracted Keywords\n",
    "\n",
    "With the keyword extractions functions implemented let's define a **few short example texts** which will be used below for keyword extraction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "id": "ZAs0XolbEQNh"
   },
   "outputs": [],
   "source": [
    "texts = [\n",
    "    # Dartmouth Workshop\n",
    "    # https://en.wikipedia.org/wiki/Dartmouth_workshop\n",
    "    (\n",
    "        \"The Dartmouth Summer Research Project on Artificial Intelligence was \"\n",
    "        \"a 1956 summer workshop widely considered to be the founding event of \"\n",
    "        \"artificial intelligence as a field. The project lasted approximately \"\n",
    "        \"six to eight weeks and was essentially an extended brainstorming \"\n",
    "        \"session. Eleven mathematicians and scientists originally planned to \"\n",
    "        \"attend; not all of them attended, but more than ten others came for \"\n",
    "        \"short times.\"\n",
    "    ),\n",
    "    # Abstract TextRank Paper\n",
    "    (\n",
    "        \"In this paper, we introduce TextRank ‚Äì a graph-based ranking model \" \n",
    "        \"for text processing, and show how this model can be successfully \"\n",
    "        \"used in natural language applications. In particular, we propose \"\n",
    "        \"two innovative unsupervised methods for keyword and sentence \"\n",
    "        \"extraction, and show that the results obtained compare favorably \"\n",
    "        \"with previously published results on established benchmarks.\"\n",
    "     ),\n",
    "    # News\n",
    "    # https://www.nytimes.com/live/2021/02/09/us/trump-impeachment-trial\n",
    "    (\n",
    "        \"The House managers prosecuting former President Donald J. Trump \"\n",
    "        \"opened his Senate impeachment trial on Tuesday with a vivid and \"\n",
    "        \"graphic sequence of footage of his supporters storming the Capitol \"\n",
    "        \"last month in an effort to prevent Congress from finalizing his \"\n",
    "        \"election defeat.\\n\"\n",
    "        \"The managers wasted no time moving immediately to their most powerful \"\n",
    "        \"evidence: the explicit visual record of the deadly Capitol siege \"\n",
    "        \"that threatened the lives of former Vice President Mike Pence and \"\n",
    "        \"members of both houses of Congress juxtaposed against Mr. Trump‚Äôs \"\n",
    "        \"own words encouraging members of the mob at a rally beforehand.\\n\"\n",
    "        \"The scenes of mayhem and violence ‚Äî punctuated by expletives rarely \"\n",
    "        \"heard on the floor of the Senate ‚Äî highlighted the drama of the \"\n",
    "        \"trial in gut-punching fashion for the senators who lived through \"\n",
    "        \"the events barely a month ago and now sit as quasi-jurors. On the \"\n",
    "        \"screens, they saw enraged extremists storming barricades, beating \"\n",
    "        \"police officers, setting up a gallows and yelling, ‚ÄúTake the \"\n",
    "        \"building,‚Äù ‚ÄúFight for Trump‚Äù and ‚ÄúPence is a traitor! Traitor Pence!‚Äù\"\n",
    "    ),\n",
    "    # Recipe\n",
    "    # https://www.nytimes.com/2021/02/08/dining/birria-recipes.html\n",
    "    (\n",
    "        \"You go to Birrieria Nochistl√°n for the Moreno family‚Äôs \"\n",
    "        \"Zacatecan-style birria ‚Äî a big bowl of hot goat meat submerged \"\n",
    "        \"in a dark pool of its own concentrated cooking juices.\\n\"\n",
    "        \"Right out of the pot, the steamed meat isn‚Äôt just tender, but \"\n",
    "        \"in places deliciously sticky, smudged with chile adobo, falling \"\n",
    "        \"apart, barely even connected to the bone. It comes with thick, \"\n",
    "        \"soft tortillas, made to order, and a vibrant salsa roja. \"\n",
    "        \"The Moreno family has been serving birria exactly like this for \"\n",
    "        \"about 20 years.\\n\"\n",
    "        \"‚ÄúSometimes I think we should update our menu,‚Äù said Rosio Moreno, \"\n",
    "        \"23, whose parents started the business out of their home in East \"\n",
    "        \"Los Angeles. ‚ÄúBut we don‚Äôt want to change the way we do things \"\n",
    "        \"because of the hype.‚Äù\"\n",
    "    ),\n",
    "    # The text within this notebook:\n",
    "    (\n",
    "        \"In a pervious quick tip we looked at pke as a replacement for Gensim's \"\n",
    "        \"recently removed keyword extraction module. pke comes with batteries \"\n",
    "        \"included: It has preprocessing build in, supports non-English languages, \"\n",
    "        \"and provides a wide range of keyword extraction methods: statistical, \"\n",
    "        \"graph-based, and supervised.\\n\" \n",
    "        \"This makes pke a great choice to get started with keyword extraction, \"\n",
    "        \"experiment with different methods and generate baselines to improve upon.\\n\"\n",
    "        \"But what if the required performance is not met by these classical methods?\\n\"\n",
    "        \"Newly-developed auspicious extraction methods fall into the category of \"\n",
    "        \"neural keyword extraction.\\n\"\n",
    "        \"The methods often utilise sequence-to-sequence models based on recurrent \"\n",
    "        \"neural networks (RNNs) or Long Short-Term Memory (LSTM). Their objective \"\n",
    "        \"is to transform a sequence of input words (the given document) into an \"\n",
    "        \"abstract intermediate representation and generate a sequence of keywords \"\n",
    "        \"from it.\\n\"\n",
    "        \"These models do not use words or phrases directly, which enables them to \"\n",
    "        \"generate unseen keywords as well. This is called keyword generation and \"\n",
    "        \"combines abstractive as well as extractive keywords.\\n\"\n",
    "        \"They report impressive performance increases over the classical \"\n",
    "        \"extraction methods.\\n\"\n",
    "        \"However, training such models requires a large collection of documents \"\n",
    "        \"and annotated keywords, since the final training step is usually \"\n",
    "        \"supervised.\\n\"\n",
    "        \"In addition, many of the models' repositories are not well maintained \"\n",
    "        \"which makes it difficult to train them, especially on different \"\n",
    "        \"languages or domains.\\n\"\n",
    "        \"This leaves the quesition of how neural keyword extraction can already \"\n",
    "        \"be used today.\\n\"\n",
    "        \"Follow the below sections in this notebook to learn how to use two \"\n",
    "        \"approaches in the neural keyword extraction category and how they \"\n",
    "        \"compare to classical extraction methods.\"\n",
    "    ),\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The next cell extracts keywords from the example texts using selected algorithms. \n",
    "\n",
    "Note that the **BART-based method extracts fewer keywords** than the other methods. This is likely due to the training data which does not necessarily contain many keywords per example. \n",
    "The other extraction methods either require the number of keywords to be specified directly or a threshold to a (similarity) score.\n",
    "\n",
    "Another interesting observation is that the keywords generated by the BART-based method do not necessarily appear in the text. This means that such models are able to generate **abstractive keywords** next to **extractive** keywords. This however, might not always be desirable and can be problematic if the generated abstractive keywords are of insufficient quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "RFgnm9mWEQNj",
    "outputId": "90473f59-16f4-4bb6-9de9-21c7a817a358"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:LoadFile._df_counts is hard coded to /usr/local/lib/python3.7/dist-packages/pke/models/df-semeval2010.tsv.gz\n",
      "WARNING:root:Candidates are generated using 0.33-top\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>KeyBERT</th>\n",
       "      <th>BART-based</th>\n",
       "      <th>TFIDF</th>\n",
       "      <th>YAKE</th>\n",
       "      <th>TextRank</th>\n",
       "      <th>TopicRank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>field project</td>\n",
       "      <td>Dartmouth University</td>\n",
       "      <td>artificial</td>\n",
       "      <td>dartmouth summer research</td>\n",
       "      <td>summer research</td>\n",
       "      <td>artificial intelligence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>artificial</td>\n",
       "      <td>Artificial intelligence</td>\n",
       "      <td>artificial intelligence</td>\n",
       "      <td>summer research project</td>\n",
       "      <td>artificial intelligence</td>\n",
       "      <td>mathematicians</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>originally planned</td>\n",
       "      <td></td>\n",
       "      <td>intelligence</td>\n",
       "      <td>1956 summer workshop</td>\n",
       "      <td>summer</td>\n",
       "      <td>field</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>intelligence</td>\n",
       "      <td></td>\n",
       "      <td>summer</td>\n",
       "      <td>summer workshop widely</td>\n",
       "      <td>brainstorming</td>\n",
       "      <td>event</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>summer research</td>\n",
       "      <td></td>\n",
       "      <td>dartmouth summer</td>\n",
       "      <td>artificial intelligence</td>\n",
       "      <td>short</td>\n",
       "      <td>extended brainstorming session</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>scientists originally</td>\n",
       "      <td></td>\n",
       "      <td>dartmouth summer research</td>\n",
       "      <td>workshop widely considered</td>\n",
       "      <td></td>\n",
       "      <td>project</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>dartmouth</td>\n",
       "      <td></td>\n",
       "      <td>summer research</td>\n",
       "      <td>dartmouth summer</td>\n",
       "      <td></td>\n",
       "      <td>scientists</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>brainstorming session</td>\n",
       "      <td></td>\n",
       "      <td>summer research project</td>\n",
       "      <td>summer research</td>\n",
       "      <td></td>\n",
       "      <td>weeks</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>project lasted</td>\n",
       "      <td></td>\n",
       "      <td>1956</td>\n",
       "      <td>research project</td>\n",
       "      <td></td>\n",
       "      <td>dartmouth summer research project</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>dartmouth summer</td>\n",
       "      <td></td>\n",
       "      <td>1956 summer</td>\n",
       "      <td>1956 summer</td>\n",
       "      <td></td>\n",
       "      <td>summer workshop</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 KeyBERT  ...                          TopicRank\n",
       "0          field project  ...            artificial intelligence\n",
       "1             artificial  ...                     mathematicians\n",
       "2     originally planned  ...                              field\n",
       "3           intelligence  ...                              event\n",
       "4        summer research  ...     extended brainstorming session\n",
       "5  scientists originally  ...                            project\n",
       "6              dartmouth  ...                         scientists\n",
       "7  brainstorming session  ...                              weeks\n",
       "8         project lasted  ...  dartmouth summer research project\n",
       "9       dartmouth summer  ...                    summer workshop\n",
       "\n",
       "[10 rows x 6 columns]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:LoadFile._df_counts is hard coded to /usr/local/lib/python3.7/dist-packages/pke/models/df-semeval2010.tsv.gz\n",
      "WARNING:root:Candidates are generated using 0.33-top\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>KeyBERT</th>\n",
       "      <th>BART-based</th>\n",
       "      <th>TFIDF</th>\n",
       "      <th>YAKE</th>\n",
       "      <th>TextRank</th>\n",
       "      <th>TopicRank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>unsupervised methods</td>\n",
       "      <td>Text</td>\n",
       "      <td>results</td>\n",
       "      <td>natural language applications</td>\n",
       "      <td>sentence extraction</td>\n",
       "      <td>model</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>processing model</td>\n",
       "      <td>Computers and the Internet</td>\n",
       "      <td>introduce</td>\n",
       "      <td>based ranking model</td>\n",
       "      <td>text processing</td>\n",
       "      <td>results</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>ranking</td>\n",
       "      <td></td>\n",
       "      <td>introduce textrank</td>\n",
       "      <td>introduce textrank</td>\n",
       "      <td>unsupervised</td>\n",
       "      <td>keyword</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>graph based</td>\n",
       "      <td></td>\n",
       "      <td>textrank</td>\n",
       "      <td>based ranking</td>\n",
       "      <td>language</td>\n",
       "      <td>innovative unsupervised methods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>language applications</td>\n",
       "      <td></td>\n",
       "      <td>based</td>\n",
       "      <td>text processing</td>\n",
       "      <td></td>\n",
       "      <td>sentence extraction</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>text</td>\n",
       "      <td></td>\n",
       "      <td>based ranking</td>\n",
       "      <td>language applications</td>\n",
       "      <td></td>\n",
       "      <td>text processing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>paper introduce</td>\n",
       "      <td></td>\n",
       "      <td>based ranking model</td>\n",
       "      <td>successfully used</td>\n",
       "      <td></td>\n",
       "      <td>graph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>sentence extraction</td>\n",
       "      <td></td>\n",
       "      <td>ranking</td>\n",
       "      <td>natural language</td>\n",
       "      <td></td>\n",
       "      <td>natural language applications</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>extraction results</td>\n",
       "      <td></td>\n",
       "      <td>ranking model</td>\n",
       "      <td>ranking model</td>\n",
       "      <td></td>\n",
       "      <td>particular</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>based ranking</td>\n",
       "      <td></td>\n",
       "      <td>text processing</td>\n",
       "      <td>textrank</td>\n",
       "      <td></td>\n",
       "      <td>textrank</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 KeyBERT  ...                        TopicRank\n",
       "0   unsupervised methods  ...                            model\n",
       "1       processing model  ...                          results\n",
       "2                ranking  ...                          keyword\n",
       "3            graph based  ...  innovative unsupervised methods\n",
       "4  language applications  ...              sentence extraction\n",
       "5                   text  ...                  text processing\n",
       "6        paper introduce  ...                            graph\n",
       "7    sentence extraction  ...    natural language applications\n",
       "8     extraction results  ...                       particular\n",
       "9          based ranking  ...                         textrank\n",
       "\n",
       "[10 rows x 6 columns]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:LoadFile._df_counts is hard coded to /usr/local/lib/python3.7/dist-packages/pke/models/df-semeval2010.tsv.gz\n",
      "WARNING:root:Candidates are generated using 0.33-top\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>KeyBERT</th>\n",
       "      <th>BART-based</th>\n",
       "      <th>TFIDF</th>\n",
       "      <th>YAKE</th>\n",
       "      <th>TextRank</th>\n",
       "      <th>TopicRank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>trump</td>\n",
       "      <td>Donald Trump</td>\n",
       "      <td>pence</td>\n",
       "      <td>former president donald</td>\n",
       "      <td>capitol last</td>\n",
       "      <td>senate impeachment trial</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>mr trump</td>\n",
       "      <td>US Politics</td>\n",
       "      <td>trump</td>\n",
       "      <td>house managers prosecuting</td>\n",
       "      <td>j. trump</td>\n",
       "      <td>members</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>house managers</td>\n",
       "      <td>Impeachment</td>\n",
       "      <td>managers</td>\n",
       "      <td>prosecuting former president</td>\n",
       "      <td>enraged extremists</td>\n",
       "      <td>congress</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>fight trump</td>\n",
       "      <td>House of Representatives</td>\n",
       "      <td>president</td>\n",
       "      <td>former vice president</td>\n",
       "      <td>own words</td>\n",
       "      <td>capitol last month</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>mike pence</td>\n",
       "      <td>Senate</td>\n",
       "      <td>senate</td>\n",
       "      <td>capitol last month</td>\n",
       "      <td>powerful evidence</td>\n",
       "      <td>house managers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>prosecuting</td>\n",
       "      <td>Congress</td>\n",
       "      <td>storming</td>\n",
       "      <td>vice president mike</td>\n",
       "      <td>election defeat</td>\n",
       "      <td>pence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>senate highlighted</td>\n",
       "      <td></td>\n",
       "      <td>capitol</td>\n",
       "      <td>president mike pence</td>\n",
       "      <td>graphic sequence</td>\n",
       "      <td>former vice president mike pence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>opened senate</td>\n",
       "      <td></td>\n",
       "      <td>members</td>\n",
       "      <td>managers prosecuting former</td>\n",
       "      <td>house managers</td>\n",
       "      <td>mayhem</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>managers prosecuting</td>\n",
       "      <td></td>\n",
       "      <td>congress</td>\n",
       "      <td>senate impeachment trial</td>\n",
       "      <td>capitol</td>\n",
       "      <td>graphic sequence</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>impeachment trial</td>\n",
       "      <td></td>\n",
       "      <td>traitor</td>\n",
       "      <td>president donald</td>\n",
       "      <td>president</td>\n",
       "      <td>trial</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                KeyBERT  ...                         TopicRank\n",
       "0                 trump  ...          senate impeachment trial\n",
       "1              mr trump  ...                           members\n",
       "2        house managers  ...                          congress\n",
       "3           fight trump  ...                capitol last month\n",
       "4            mike pence  ...                    house managers\n",
       "5           prosecuting  ...                             pence\n",
       "6    senate highlighted  ...  former vice president mike pence\n",
       "7         opened senate  ...                            mayhem\n",
       "8  managers prosecuting  ...                  graphic sequence\n",
       "9     impeachment trial  ...                             trial\n",
       "\n",
       "[10 rows x 6 columns]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:LoadFile._df_counts is hard coded to /usr/local/lib/python3.7/dist-packages/pke/models/df-semeval2010.tsv.gz\n",
      "WARNING:root:Candidates are generated using 0.33-top\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>KeyBERT</th>\n",
       "      <th>BART-based</th>\n",
       "      <th>TFIDF</th>\n",
       "      <th>YAKE</th>\n",
       "      <th>TextRank</th>\n",
       "      <th>TopicRank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>hot goat</td>\n",
       "      <td>Restaurant</td>\n",
       "      <td>moreno</td>\n",
       "      <td>concentrated cooking juices</td>\n",
       "      <td>concentrated cooking</td>\n",
       "      <td>moreno family</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>deliciously sticky</td>\n",
       "      <td>Los Angeles</td>\n",
       "      <td>moreno family</td>\n",
       "      <td>birrieria nochistl√°n</td>\n",
       "      <td>goat meat</td>\n",
       "      <td>style birria</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>goat meat</td>\n",
       "      <td></td>\n",
       "      <td>family</td>\n",
       "      <td>hot goat meat</td>\n",
       "      <td>big bowl</td>\n",
       "      <td>zacatecan</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>places deliciously</td>\n",
       "      <td></td>\n",
       "      <td>birria</td>\n",
       "      <td>goat meat submerged</td>\n",
       "      <td>style birria</td>\n",
       "      <td>places</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>nochistl√°n moreno</td>\n",
       "      <td></td>\n",
       "      <td>meat</td>\n",
       "      <td>cooking juices</td>\n",
       "      <td>birrieria nochistl√°n</td>\n",
       "      <td>sticky</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>meat isn</td>\n",
       "      <td></td>\n",
       "      <td>birrieria</td>\n",
       "      <td>big bowl</td>\n",
       "      <td>los</td>\n",
       "      <td>big bowl</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rosio moreno</td>\n",
       "      <td></td>\n",
       "      <td>birrieria nochistl√°n</td>\n",
       "      <td>hot goat</td>\n",
       "      <td>salsa</td>\n",
       "      <td>thick</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>vibrant salsa</td>\n",
       "      <td></td>\n",
       "      <td>nochistl√°n</td>\n",
       "      <td>dark pool</td>\n",
       "      <td>moreno</td>\n",
       "      <td>hot goat meat</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>birrieria</td>\n",
       "      <td></td>\n",
       "      <td>zacatecan</td>\n",
       "      <td>concentrated cooking</td>\n",
       "      <td>meat</td>\n",
       "      <td>home</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>birria exactly</td>\n",
       "      <td></td>\n",
       "      <td>style birria</td>\n",
       "      <td>goat meat</td>\n",
       "      <td>birria</td>\n",
       "      <td>soft tortillas</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              KeyBERT   BART-based  ...              TextRank       TopicRank\n",
       "0            hot goat   Restaurant  ...  concentrated cooking   moreno family\n",
       "1  deliciously sticky  Los Angeles  ...             goat meat    style birria\n",
       "2           goat meat               ...              big bowl       zacatecan\n",
       "3  places deliciously               ...          style birria          places\n",
       "4   nochistl√°n moreno               ...  birrieria nochistl√°n          sticky\n",
       "5            meat isn               ...                   los        big bowl\n",
       "6        rosio moreno               ...                 salsa           thick\n",
       "7       vibrant salsa               ...                moreno   hot goat meat\n",
       "8           birrieria               ...                  meat            home\n",
       "9      birria exactly               ...                birria  soft tortillas\n",
       "\n",
       "[10 rows x 6 columns]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:LoadFile._df_counts is hard coded to /usr/local/lib/python3.7/dist-packages/pke/models/df-semeval2010.tsv.gz\n",
      "WARNING:root:Candidates are generated using 0.33-top\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>KeyBERT</th>\n",
       "      <th>BART-based</th>\n",
       "      <th>TFIDF</th>\n",
       "      <th>YAKE</th>\n",
       "      <th>TextRank</th>\n",
       "      <th>TopicRank</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>methods unable</td>\n",
       "      <td>Search Engines</td>\n",
       "      <td>pke</td>\n",
       "      <td>pervious quick tip</td>\n",
       "      <td>extraction methods</td>\n",
       "      <td>keyword extraction module</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>keyword extraction</td>\n",
       "      <td>Gensim</td>\n",
       "      <td>keyword extraction</td>\n",
       "      <td>recently removed keyword</td>\n",
       "      <td>- english</td>\n",
       "      <td>pke</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>languages provdes</td>\n",
       "      <td></td>\n",
       "      <td>extraction</td>\n",
       "      <td>keyword extraction module</td>\n",
       "      <td>wide range</td>\n",
       "      <td>different methods</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>pervious quick</td>\n",
       "      <td></td>\n",
       "      <td>methods</td>\n",
       "      <td>recently removed</td>\n",
       "      <td>extraction</td>\n",
       "      <td>experiment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>improve methods</td>\n",
       "      <td></td>\n",
       "      <td>keyword</td>\n",
       "      <td>removed keyword extraction</td>\n",
       "      <td>quick</td>\n",
       "      <td>statistical</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>replacement gensim</td>\n",
       "      <td></td>\n",
       "      <td>pervious</td>\n",
       "      <td>pervious quick</td>\n",
       "      <td>-</td>\n",
       "      <td>replacement</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>methods generate</td>\n",
       "      <td></td>\n",
       "      <td>pervious quick</td>\n",
       "      <td>quick tip</td>\n",
       "      <td>methods</td>\n",
       "      <td>gensim</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>extraction module</td>\n",
       "      <td></td>\n",
       "      <td>pervious quick tip</td>\n",
       "      <td>keyword extraction</td>\n",
       "      <td>great</td>\n",
       "      <td>graph</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>pke comes</td>\n",
       "      <td></td>\n",
       "      <td>quick tip</td>\n",
       "      <td>extraction module</td>\n",
       "      <td></td>\n",
       "      <td>great choice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>pke replacement</td>\n",
       "      <td></td>\n",
       "      <td>tip</td>\n",
       "      <td>gensim</td>\n",
       "      <td></td>\n",
       "      <td>wide range</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              KeyBERT  ...                  TopicRank\n",
       "0      methods unable  ...  keyword extraction module\n",
       "1  keyword extraction  ...                        pke\n",
       "2   languages provdes  ...          different methods\n",
       "3      pervious quick  ...                 experiment\n",
       "4     improve methods  ...                statistical\n",
       "5  replacement gensim  ...                replacement\n",
       "6    methods generate  ...                     gensim\n",
       "7   extraction module  ...                      graph\n",
       "8           pke comes  ...               great choice\n",
       "9     pke replacement  ...                 wide range\n",
       "\n",
       "[10 rows x 6 columns]"
      ]
     },
     "metadata": {
      "tags": []
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Compare the keywords extracted by the given algorithms\n",
    "selected_algorithms = ['KeyBERT', 'BART-based', 'TFIDF','YAKE', 'TextRank', 'TopicRank']\n",
    "\n",
    "for text in texts:\n",
    "    compare_keyword_extraction_algorithms(text, selection=selected_algorithms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ev2r_xMKCPJ5"
   },
   "source": [
    "## üßë‚Äçüî¨ Try it yourself!\n",
    "\n",
    "**Task**: \n",
    "\n",
    "1. Insert your own text that you would like to extract keywords from\n",
    "2. Select the desired keyword extraction methods\n",
    "3. Extract keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "PHbZr2a8CPJ7"
   },
   "outputs": [],
   "source": [
    "# Task 1: Add your own input text to e\n",
    "text = \"Replace this string rambling on about keyword extraction and how great it is with your own text\"\n",
    "\n",
    "# Task 2: Select the desired keyword extraction methods you want to compare\n",
    "selected_algorithms = ['KeyBERT', 'BART-based', 'TFIDF', 'YAKE', 'TextRank', 'TopicRank']\n",
    "\n",
    "# Task 3: Execute this cell to compare the extracted keywords\n",
    "compare_keyword_extraction_algorithms(text, selection=selected_algorithms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9wfoRIn1zPQD"
   },
   "source": [
    "## Summary\n",
    "\n",
    "This notebook gave a brief overview of neural keyword extraction.\n",
    "\n",
    "* Neural keyword extraction is a relatively novel approach to keyword extraction that utilizes large language models. It is not limited to *extracting* keywords but able to generate **abstractive keywords**.\n",
    "\n",
    "* If an annotated dataset is available or if the problem domain is closely related to one of the existing dataset and it's possible to invest some time into training a model yourself, neural keyword extraction **promises great performance**.\n",
    "\n",
    "* We further presented two approaches that are readily available: **KeyBERT** uses the **similarity** between word and text **embeddings** to find keywords which best describe a text and an **end-to-end neural keyword extraction model** based on BART.\n",
    "\n",
    "üëâ For now, we recommend to first try `pke` to establish a solid baseline and explore the mentioned extraction methods as an addition.\n",
    "\n",
    "\n",
    "## Resources\n",
    "\n",
    "### üìö Libraries & Packages\n",
    "\n",
    "* [**KeyBERT**](https://github.com/MaartenGr/KeyBERT): Keyword extraction method, that chooses keywords whose word embeddings are most similar to embeddings of the entire text\n",
    "* **BART-based neural keyword extraction** models [trained on the KPTimes dataset](https://huggingface.co/ankur310794/bart-base-keyphrase-generation-kpTimes) or [OpenKP dataset](https://huggingface.co/ankur310794/bart-base-keyphrase-generation-openkp) on ü§ó Model Hub.\n",
    "* [**`pke`** python keyphrase extraction](https://github.com/boudinfl/pke): Neat library implementing amongst others TF-IDF, YAKE, KPMiner, TextRank, SingleRank, TopicRank, TopologicalPageRank, PositionRank, MultipartiteRank, KEA, and WINGNUS. Uses GPLv3 licence.[[documentation](https://boudinfl.github.io/pke/)]\n",
    "* [**YAKE**](https://github.com/LIAAD/yake): An alternative implementation from the authors of the YAKE paper.\n",
    "\n",
    "\n",
    "### üìÑ Overview Papers\n",
    "\n",
    "* *Keyword extraction: a review of methods and approaches* by Slobodan Beliga (2014)\n",
    " [[paper](http://langnet.uniri.hr/papers/beliga/Beliga_KeywordExtraction_a_review_of_methods_and_approaches.pdf)]\n",
    "* *A Review of Keyphrase Extraction* by Eirini Papagiannopoulou and Grigorios Tsoumakas (2019) [[paper](https://arxiv.org/pdf/1905.05044)]\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "Ak0IG15fsUVV"
   ],
   "name": "Neural Keyword Extraction Tip-of-the-week.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
