{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Quantizing GPT2 to reduce costs and latency ðŸ’µðŸ’ª"
      ],
      "metadata": {
        "id": "ztE-GnNZ1y9M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## System config âš™ï¸\n",
        "To install required dependencies"
      ],
      "metadata": {
        "id": "btmL4bGy3CmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install -q torch==1.7.0+cpu torchvision==0.8.1+cpu -f https://download.pytorch.org/whl/torch_stable.html\n",
        "!pip install -q onnxruntime==1.8.0\n",
        "!pip install -q transformers==4.3.1 datasets\n",
        "!pip install -q onnx onnxconverter_common psutil pytz pandas py-cpuinfo py3nvml coloredlogs"
      ],
      "metadata": {
        "id": "ilCIfc2ZJa9h"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Imports and general settings ðŸ”§"
      ],
      "metadata": {
        "id": "SH0cDZiS3Eim"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import Tensor\n",
        "from torch.nn import functional as F\n",
        "from torch.nn import CrossEntropyLoss"
      ],
      "metadata": {
        "id": "7lSwHrli4YZR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# In this example we will be quantizing the Dutch GPT2-small model\n",
        "\n",
        "model_ckpt = \"ml6team/gpt2-small-dutch-finetune-oscar\"\n",
        "device = torch.device(\"cpu\")"
      ],
      "metadata": {
        "id": "c-_reRYJKWEt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "\n",
        "# Create a cache directory to store pretrained model.\n",
        "cache_dir = os.path.join(\".\", \"cache_models\")\n",
        "if not os.path.exists(cache_dir):\n",
        "    os.makedirs(cache_dir)"
      ],
      "metadata": {
        "id": "6xE4X1EeD_8A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Quantization ðŸ¤"
      ],
      "metadata": {
        "id": "LBq8-TvH3GUt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Convert HF model to ONNX"
      ],
      "metadata": {
        "id": "RD8IPp_nPZtR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from onnxruntime.transformers.gpt2_helper import Gpt2Helper, MyGPT2LMHeadModel"
      ],
      "metadata": {
        "id": "7leojL_rD7J7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#Load in the model config\n",
        "from transformers import AutoConfig\n",
        "\n",
        "model_name_or_path = model_ckpt\n",
        "config = AutoConfig.from_pretrained(model_name_or_path, cache_dir=cache_dir)"
      ],
      "metadata": {
        "id": "qQnD7NBv3ziF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Instantiate the model\n",
        "model_regular = MyGPT2LMHeadModel.from_pretrained(model_name_or_path, config=config, cache_dir=cache_dir)"
      ],
      "metadata": {
        "id": "jtlWsy6b33vJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Activate eval mode to for example deactivate Dropout, and transfer to the device\n",
        "model_regular.eval().to(device)"
      ],
      "metadata": {
        "id": "v-yfeu1G4TQB"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Obtain parameters for downstream usage\n",
        "num_attention_heads = model_regular.config.n_head\n",
        "hidden_size = model_regular.config.n_embd\n",
        "num_layer = model_regular.config.n_layer"
      ],
      "metadata": {
        "id": "b-qSziMH3bia"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Export to ONNX binary\n",
        "Gpt2Helper.export_onnx(model_regular, device, \"gpt2_regular.onnx\")"
      ],
      "metadata": {
        "id": "zd4hQkm1EEbC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Optimize before quantization\n",
        "To for example perform step fusing in the model graph"
      ],
      "metadata": {
        "id": "1DZJ0A6PE-JQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Gpt2Helper.optimize_onnx(\n",
        "    \"gpt2_regular.onnx\",\n",
        "    \"gpt2_regular_opt.onnx\",\n",
        "    False,\n",
        "    model_regular.config.num_attention_heads,\n",
        "    model_regular.config.hidden_size)"
      ],
      "metadata": {
        "id": "y6nXYluK5Pvu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quantize the models"
      ],
      "metadata": {
        "id": "VKYdr9Bb51mT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from onnxruntime.transformers.quantize_helper import QuantizeHelper"
      ],
      "metadata": {
        "id": "Oh-fKkycHhMn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "QuantizeHelper.quantize_onnx_model(\n",
        "    \"gpt2_regular_opt.onnx\",\n",
        "    \"gpt2_regular_opt_int8.onnx\")"
      ],
      "metadata": {
        "id": "4RQQyeAV56kk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Evaluate the quantized model ðŸ”Ž"
      ],
      "metadata": {
        "id": "7byDq3ZJQQRb"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sampling code\n",
        "Since we don't want to perform greedy decoding, but use a more sophisticated sampling strategy, we had to **coughs* borrow some code from [this HF repo page](https://github.com/huggingface/transformers/blob/main/src/transformers/generation_utils.py)."
      ],
      "metadata": {
        "id": "xObR--ZA-vIA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def top_k_top_p_filtering(\n",
        "    logits: Tensor,\n",
        "    top_k: int = 0,\n",
        "    top_p: float = 1.0,\n",
        "    filter_value: float = -float(\"Inf\"),\n",
        "    min_tokens_to_keep: int = 1,\n",
        ") -> Tensor:\n",
        "    \"\"\"Filter a distribution of logits using top-k and/or nucleus (top-p) filtering\n",
        "    Args:\n",
        "        logits: logits distribution shape (batch size, vocabulary size)\n",
        "        if top_k > 0: keep only top k tokens with highest probability (top-k filtering).\n",
        "        if top_p < 1.0: keep the top tokens with cumulative probability >= top_p (nucleus filtering).\n",
        "            Nucleus filtering is described in Holtzman et al. (http://arxiv.org/abs/1904.09751)\n",
        "        Make sure we keep at least min_tokens_to_keep per batch example in the output\n",
        "    From: https://gist.github.com/thomwolf/1a5a29f6962089e871b94cbd09daf317\n",
        "    \"\"\"\n",
        "    if top_k > 0:\n",
        "        top_k = min(max(top_k, min_tokens_to_keep), logits.size(-1))  # Safety check\n",
        "        # Remove all tokens with a probability less than the last token of the top-k\n",
        "        indices_to_remove = logits < torch.topk(logits, top_k)[0][..., -1, None]\n",
        "        logits[indices_to_remove] = filter_value\n",
        "\n",
        "    if top_p < 1.0:\n",
        "        sorted_logits, sorted_indices = torch.sort(logits, descending=True)\n",
        "        cumulative_probs = torch.cumsum(F.softmax(sorted_logits, dim=-1), dim=-1)\n",
        "\n",
        "        # Remove tokens with cumulative probability above the threshold (token with 0 are kept)\n",
        "        sorted_indices_to_remove = cumulative_probs > top_p\n",
        "        if min_tokens_to_keep > 1:\n",
        "            # Keep at least min_tokens_to_keep (set to min_tokens_to_keep-1 because we add the first one below)\n",
        "            sorted_indices_to_remove[..., :min_tokens_to_keep] = 0\n",
        "        # Shift the indices to the right to keep also the first token above the threshold\n",
        "        sorted_indices_to_remove[..., 1:] = sorted_indices_to_remove[..., :-1].clone()\n",
        "        sorted_indices_to_remove[..., 0] = 0\n",
        "\n",
        "        # scatter sorted tensors to original indexing\n",
        "        indices_to_remove = sorted_indices_to_remove.scatter(1, sorted_indices, sorted_indices_to_remove)\n",
        "        logits[indices_to_remove] = filter_value\n",
        "    return logits"
      ],
      "metadata": {
        "id": "YKlMzlGIEY90"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Inference helper code\n",
        "These helper methods have been copied from [this notebook](https://github.com/microsoft/onnxruntime/blob/master/onnxruntime/python/tools/transformers/notebooks/Inference_GPT2_with_OnnxRuntime_on_CPU.ipynb) from the ONNXRuntime github repo."
      ],
      "metadata": {
        "id": "Ppg-8P7FQwM1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy\n",
        "from transformers import AutoTokenizer"
      ],
      "metadata": {
        "id": "q6baqCZr8GgZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_tokenizer(model_name_or_path, cache_dir):\n",
        "    # Fetch and prepare the tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n",
        "    tokenizer.padding_side = \"left\"\n",
        "    tokenizer.pad_token = tokenizer.eos_token\n",
        "    #okenizer.add_special_tokens({'pad_token': '[PAD]'})\n",
        "    return tokenizer\n",
        "\n",
        "def get_example_inputs(prompt_text):  \n",
        "    # Prepare the input text for furhter processing  \n",
        "    tokenizer = get_tokenizer(model_name_or_path, cache_dir)\n",
        "    encodings_dict = tokenizer.batch_encode_plus(prompt_text, padding=True)\n",
        "\n",
        "    input_ids = torch.tensor(encodings_dict['input_ids'], dtype=torch.int64)\n",
        "    attention_mask = torch.tensor(encodings_dict['attention_mask'], dtype=torch.float32)\n",
        "    position_ids = (attention_mask.long().cumsum(-1) - 1)\n",
        "    position_ids.masked_fill_(position_ids < 0, 0)\n",
        "\n",
        "    #Empty Past State for generating first word\n",
        "    empty_past = []\n",
        "    batch_size = input_ids.size(0)\n",
        "    sequence_length = input_ids.size(1)\n",
        "    past_shape = [2, batch_size, num_attention_heads, 0, hidden_size // num_attention_heads]\n",
        "    for i in range(num_layer):\n",
        "        empty_past.append(torch.empty(past_shape).type(torch.float32).to(device))\n",
        "       \n",
        "    return input_ids, attention_mask, position_ids, empty_past"
      ],
      "metadata": {
        "id": "gRfFQRCx6V00"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def regular_inference_with_io_binding(session, config, input_ids, position_ids, attention_mask, past):\n",
        "    # Helper method to perform ORT session inference with IO Binding\n",
        "    output_shapes = Gpt2Helper.get_output_shapes(batch_size=input_ids.size(0),\n",
        "                                                 past_sequence_length=past[0].size(3),\n",
        "                                                 sequence_length=input_ids.size(1),\n",
        "                                                 config=config)\n",
        "    output_buffers = Gpt2Helper.get_output_buffers(output_shapes, device)\n",
        "\n",
        "    io_binding = Gpt2Helper.prepare_io_binding(session, input_ids, position_ids, attention_mask, past,\n",
        "                                               output_buffers, output_shapes)\n",
        "    session.run_with_iobinding(io_binding)\n",
        "\n",
        "    outputs = Gpt2Helper.get_outputs_from_io_binding_buffer(session, output_buffers, output_shapes,\n",
        "                                                            return_numpy=False)\n",
        "    return outputs"
      ],
      "metadata": {
        "id": "cH7llKIjhuAb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def regular_test_generation(tokenizer, input_text, ort_session=None, num_tokens_to_produce = 30, top_k=50, top_p=0.95, do_sample=False, temperature=1.0):\n",
        "    use_onnxruntime = (ort_session is not None)\n",
        "    print(\"Text generation using\", \"OnnxRuntime\" if use_onnxruntime else \"PyTorch\", \"...\")\n",
        "    eos_token_id = tokenizer.eos_token_id\n",
        "    \n",
        "    input_ids, attention_mask, position_ids, past = get_example_inputs(input_text)\n",
        "    batch_size = input_ids.size(0)\n",
        "\n",
        "    has_eos = torch.zeros(batch_size, dtype=torch.bool)\n",
        "\n",
        "    all_token_ids = input_ids.clone()\n",
        "\n",
        "    for step in range(num_tokens_to_produce):\n",
        "        outputs = regular_inference_with_io_binding(ort_session, config, input_ids, position_ids, attention_mask, past)\n",
        "\n",
        "        # Get next logits\n",
        "        next_token_logits = outputs[0][:, -1, :]\n",
        "\n",
        "        # Top-k sampling\n",
        "        if do_sample:\n",
        "            # Temperature (higher temperature => more likely to sample low probability tokens)\n",
        "            if temperature != 1.0:\n",
        "                    scores = next_token_logits / temperature\n",
        "            next_token_logscores = top_k_top_p_filtering(scores, top_k=top_k, top_p=top_p)\n",
        "            probs = F.softmax(next_token_logscores, dim=-1)\n",
        "            next_tokens = torch.multinomial(probs, num_samples=1).squeeze(1)\n",
        "        else:\n",
        "            # Greedy sampling\n",
        "            next_tokens = torch.argmax(next_token_logits, dim=-1)\n",
        "\n",
        "        has_eos = has_eos | (next_tokens == eos_token_id)\n",
        "        tokens_to_add = next_tokens.masked_fill(has_eos, eos_token_id)\n",
        "        all_token_ids = torch.cat([all_token_ids, tokens_to_add.unsqueeze(-1)], dim=-1)\n",
        "\n",
        "        # Update input_ids, attention_mask, position_ids and past\n",
        "        input_ids = tokens_to_add.clone().detach().reshape([batch_size, 1]).to(device)    \n",
        "        position_ids = (position_ids[:,-1] + 1).reshape(batch_size,1)\n",
        "        attention_mask = torch.cat([attention_mask, torch.ones([batch_size, 1]).type_as(attention_mask)], 1).to(device)    \n",
        "\n",
        "        past = []\n",
        "        if not use_onnxruntime:\n",
        "            past = list(outputs[1]) # past in torch output is tuple\n",
        "        else:\n",
        "            for i in range(num_layer):\n",
        "                past_i = torch.from_numpy(outputs[i + 1]) if isinstance(outputs[i + 1], numpy.ndarray) else outputs[i + 1].clone().detach()\n",
        "                past.append(past_i.to(device))\n",
        "\n",
        "        if torch.all(has_eos):\n",
        "            break\n",
        "\n",
        "    for i, output in enumerate(all_token_ids):\n",
        "        print(\"------------\")\n",
        "        print(tokenizer.decode(output, skip_special_tokens=True))"
      ],
      "metadata": {
        "id": "rw6QIocxJXg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Basic output quality tests"
      ],
      "metadata": {
        "id": "mEpO9ChpR-5-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from onnxruntime import InferenceSession"
      ],
      "metadata": {
        "id": "yMCrp2se69og"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = get_tokenizer(model_name_or_path, cache_dir)\n",
        "length=5"
      ],
      "metadata": {
        "id": "hWsfzmqa6vRy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "session_int8_regular = InferenceSession(\"gpt2_regular_opt_int8.onnx\")"
      ],
      "metadata": {
        "id": "8R3MmCzH6ov_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_text = ['Dit is een test om', 'Dit is een test om', 'Dit is een test om']"
      ],
      "metadata": {
        "id": "MXzhvNbwSM18"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "regular_test_generation(\n",
        "    tokenizer,\n",
        "    input_text,\n",
        "    do_sample=True,\n",
        "    top_p=0.95,\n",
        "    top_k=50,\n",
        "    temperature=0.95,\n",
        "    ort_session=session_int8_regular,\n",
        "    num_tokens_to_produce=length)"
      ],
      "metadata": {
        "id": "wqV9Fqz-7DR8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Compare the output logits"
      ],
      "metadata": {
        "id": "c5tC8kXD1ki1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import GPT2LMHeadModel, AutoConfig\n",
        "import onnxruntime\n",
        "import numpy as np\n",
        "from tqdm.notebook import tqdm"
      ],
      "metadata": {
        "id": "KQkop8cM1xXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model_name_or_path= model_ckpt"
      ],
      "metadata": {
        "id": "Zy8QUV2r1w2r"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids, attention_mask, position_ids, empty_past = get_example_inputs(prompt_text=[\"Ik zie het niet meer zitten om\"])"
      ],
      "metadata": {
        "id": "duGgTGH_2CBo"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Testing dataset\n",
        "For testing, we will select a small sample from the OSCAR dutch corpus"
      ],
      "metadata": {
        "id": "1f8O8Cr-TDZm"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"nthngdy/oscar-mini\", \"unshuffled_deduplicated_nl\", download_mode=\"force_redownload\")"
      ],
      "metadata": {
        "id": "7Ba5WKcF3858"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### HF logits"
      ],
      "metadata": {
        "id": "3tO4P1WI2r2_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "config = AutoConfig.from_pretrained(model_name_or_path, cache_dir=cache_dir)\n",
        "\n",
        "torch_model = GPT2LMHeadModel.from_pretrained(model_name_or_path, config=config, cache_dir=cache_dir)\n",
        "device = torch.device(\"cpu\")\n",
        "torch_model.eval().to(device)"
      ],
      "metadata": {
        "id": "l_N03oxu14AV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_hf_logits(input_ids, empty_past, attention_mask, position_ids):\n",
        "    with torch.no_grad():\n",
        "        torch_output = torch_model(input_ids, past_key_values=empty_past, attention_mask=attention_mask, position_ids=position_ids)\n",
        "    \n",
        "    return torch_output[0]"
      ],
      "metadata": {
        "id": "oJxLn_4-1nrI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ORT logits"
      ],
      "metadata": {
        "id": "wNiqSFAa2vb_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "onnx_model_path = \"gpt2_regular_opt_int8.onnx\"\n",
        "session = onnxruntime.InferenceSession(onnx_model_path)\n",
        "\n",
        "def get_ort_logits(input_ids, empty_past, attention_mask, position_ids):\n",
        "    ort_inputs = {'input_ids': np.ascontiguousarray(input_ids.cpu().numpy()),\n",
        "                'attention_mask' : np.ascontiguousarray(attention_mask.cpu().numpy()),\n",
        "                'position_ids': np.ascontiguousarray(position_ids.cpu().numpy())\n",
        "                }\n",
        "    for i, past_i in enumerate(empty_past):\n",
        "        ort_inputs[f'past_{i}'] = np.ascontiguousarray(past_i.cpu().numpy())\n",
        "    ort_outputs = session.run(None, ort_inputs)\n",
        "\n",
        "    return ort_outputs[0]"
      ],
      "metadata": {
        "id": "bj8sAEqk2t6Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Compare"
      ],
      "metadata": {
        "id": "YjT7b1sT278D"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "all_max_logits_diff = []\n",
        "all_mean_logits_diff = []\n",
        "all_median_logits_diff = []\n",
        "\n",
        "for line in tqdm(dataset['train'][\"text\"][:100]):\n",
        "\n",
        "    # get inputs\n",
        "    input_ids, attention_mask, position_ids, empty_past = get_example_inputs(prompt_text=[line])\n",
        "\n",
        "    # hf logits\n",
        "    hf_logits = get_hf_logits(input_ids, empty_past, attention_mask, position_ids)\n",
        "\n",
        "    # ort logits\n",
        "    ort_logits = get_ort_logits(input_ids, empty_past, attention_mask, position_ids)\n",
        "\n",
        "    # compare\n",
        "    logits_masked_diff = (hf_logits - ort_logits) * attention_mask.unsqueeze(2)\n",
        "\n",
        "    max_logits_diff = logits_masked_diff.abs().max()\n",
        "    mean_logits_diff = logits_masked_diff.abs().mean()\n",
        "    median_logits_diff = logits_masked_diff.abs().median()\n",
        "\n",
        "    all_max_logits_diff.append(max_logits_diff)\n",
        "    all_mean_logits_diff.append(mean_logits_diff)\n",
        "    all_median_logits_diff.append(median_logits_diff)"
      ],
      "metadata": {
        "id": "ZnvCUujM_Qw8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(np.mean(all_max_logits_diff))\n",
        "print(np.mean(all_mean_logits_diff))\n",
        "print(np.mean(all_median_logits_diff))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GDFDlMFX_-3t",
        "outputId": "51dd4526-9025-46b8-b06a-d85c3431cbf7"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "16.129986\n",
            "2.8550153\n",
            "2.3409908\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Perplexity\n",
        "To measure the text generation capabilities"
      ],
      "metadata": {
        "id": "Walu7I8g30H6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#Fetch a portion of the test dataset\n",
        "\n",
        "total_string= \"\\n\\n\".join(dataset['train'][\"text\"][:1000])\n",
        "encodings = tokenizer(total_string, return_tensors=\"pt\")\n",
        "\n",
        "input_ids, attention_mask, position_ids, empty_past = get_example_inputs(prompt_text=[total_string])"
      ],
      "metadata": {
        "id": "dFW8gaUS4OX7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_length = torch_model.config.n_positions\n",
        "stride = 512"
      ],
      "metadata": {
        "id": "2EVQi6-SXNSL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Torch model PPL"
      ],
      "metadata": {
        "id": "henH31p4XOht"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "nlls = []\n",
        "\n",
        "for i in tqdm(range(0, input_ids.size(1), stride)):\n",
        "    begin_loc = max(i + stride - max_length, 0)\n",
        "    end_loc = min(i + stride, input_ids.size(1))\n",
        "    trg_len = end_loc - i  # may be different from stride on last loop\n",
        "    input_ids_local = input_ids[:, begin_loc:end_loc].to(device)\n",
        "    target_ids = input_ids_local.clone()\n",
        "    target_ids[:, :-trg_len] = -100\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = torch_model(input_ids_local, labels=target_ids)\n",
        "        neg_log_likelihood = outputs[0] * trg_len\n",
        "\n",
        "    nlls.append(neg_log_likelihood)\n",
        "\n",
        "torch_ppl = torch.exp(torch.stack(nlls).sum() / end_loc)"
      ],
      "metadata": {
        "id": "soAyUpkJ3zeM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### ORT quantized PPL"
      ],
      "metadata": {
        "id": "moN17nG9XvsV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "onnx_model_path = \"gpt2_regular_opt_int8.onnx\"\n",
        "session = onnxruntime.InferenceSession(onnx_model_path)\n",
        "\n",
        "nlls = []\n",
        "for i in tqdm(range(0, input_ids.size(1), stride)):\n",
        "    begin_loc = max(i + stride - max_length, 0)\n",
        "    end_loc = min(i + stride, input_ids.size(1))\n",
        "    trg_len = end_loc - i  # may be different from stride on last loop\n",
        "\n",
        "    # Slice it up\n",
        "    input_ids_local = input_ids[:, begin_loc:end_loc].to(device)\n",
        "    attention_mask_local = attention_mask[:, begin_loc:end_loc].to(device)\n",
        "    position_ids_local = position_ids[:,:input_ids_local.size(1)].to(device)\n",
        "\n",
        "    target_ids = input_ids_local.clone()\n",
        "    target_ids[:, :-trg_len] = -100\n",
        "\n",
        "    ort_inputs = {\n",
        "        'input_ids': numpy.ascontiguousarray(input_ids_local.cpu().numpy()),\n",
        "        'attention_mask' : numpy.ascontiguousarray(attention_mask_local.cpu().numpy()),\n",
        "        'position_ids': numpy.ascontiguousarray(position_ids_local.cpu().numpy())\n",
        "        }\n",
        "\n",
        "    for i, past_i in enumerate(empty_past):\n",
        "        ort_inputs[f'past_{i}'] = numpy.ascontiguousarray(past_i.cpu().numpy())\n",
        "\n",
        "    ort_outputs = session.run(None, ort_inputs)\n",
        "    ort_outputs_logits = torch.from_numpy(ort_outputs[0])\n",
        "\n",
        "    # Calculate loss\n",
        "\n",
        "    shift_logits = ort_outputs_logits[..., :-1, :].contiguous()\n",
        "    shift_labels = target_ids[..., 1:].contiguous()\n",
        "\n",
        "    loss_fct = CrossEntropyLoss()\n",
        "    loss = loss_fct(shift_logits.view(-1, shift_logits.size(-1)), shift_labels.view(-1))\n",
        "\n",
        "    neg_log_likelihood = loss * trg_len\n",
        "\n",
        "    # print(neg_log_likelihood)\n",
        "\n",
        "    nlls.append(neg_log_likelihood)\n",
        "\n",
        "quantized_ppl = torch.exp(torch.stack(nlls).sum() / end_loc)"
      ],
      "metadata": {
        "id": "VB3cpmxJ7Fw4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Compare"
      ],
      "metadata": {
        "id": "ZJ_zA7cmYATQ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Non-quantized perplexity: {torch_ppl}\")\n",
        "print(f\"Quantized perplexity: {quantized_ppl}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IUschPH7LP7U",
        "outputId": "ecbd197f-b917-4e15-c259-d8b140d1d245"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Non-quantized perplexity: 52.991127014160156\n",
            "Quantized perplexity: 75.03434753417969\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [
        "btmL4bGy3CmW",
        "SH0cDZiS3Eim",
        "RD8IPp_nPZtR",
        "1DZJ0A6PE-JQ",
        "henH31p4XOht"
      ],
      "name": "gpt2_quantization",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}