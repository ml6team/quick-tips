{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bm6o_yqtLcta"
      },
      "outputs": [],
      "source": [
        "%load_ext pycodestyle_magic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9O-Yw-XALctd"
      },
      "outputs": [],
      "source": [
        "%pycodestyle_on"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BipCz3XbVTNe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import tensorflow as tf\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "from google.cloud import storage"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# When using Colab, you need to allow it to access your resources on GCP.\n",
        "try:\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Fill in the project-id of your GCP project and the name of \n",
        "# the GCS bucket where you want to store the checkpoints\n",
        "PROJECT = \"my_gcp_project\"\n",
        "BUCKET = \"my_gcp_bucket\""
      ],
      "metadata": {
        "id": "K7iUuPYMNKH8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oLIAQ7_VTNj"
      },
      "source": [
        "# Checkpoint helper functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvy-LB1OVTNk"
      },
      "source": [
        "This section contains the helper functions for storing and loading Tensorflow/Keras checkpoints on Google Cloud Storage. You can use these functions as a starting point and modify them for your use case.\n",
        "\n",
        "The only dependency for this boilerplate is the Python Client for Google Cloud Storage, you can find the docs here:\n",
        "\n",
        "https://googleapis.dev/python/storage/latest/index.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCOVNC6Apxd6"
      },
      "source": [
        "## Helper functions for local file system manipulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hGk935bZLctg"
      },
      "source": [
        "Helper functions for local file system manipulation. Tensorflow does not support to save a checkpoint in an archived format such as `zip` or `tar`. Archiving checkpoints before sending them to GCS can help reducing necessary storage and make the file structure on gcs simpler. It is not necessary to archive checkpoints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82lkhbqOVTNl"
      },
      "outputs": [],
      "source": [
        "def list_files(directory: str, recursive: bool = True):\n",
        "    \"\"\"Create a generator that lists all files in a directory.\n",
        "    Optionally also recusrively all list files in subdirectories.\n",
        "\n",
        "    Args:\n",
        "        directory (str): List files in this directory.\n",
        "        recursive (bool, optional): Option to recusrively list files\n",
        "        in subdirectories. Defaults to True.\n",
        "\n",
        "    Returns:\n",
        "        Generator[str]: generator of filepaths in string format.\n",
        "    \"\"\"\n",
        "    if recursive:\n",
        "        filepaths = Path(directory).rglob('*')\n",
        "    else:\n",
        "        filepaths = Path(directory).glob('*')\n",
        "\n",
        "    return (str(child) for child in filepaths if child.is_file())\n",
        "\n",
        "\n",
        "def clear_directory(directory: str):\n",
        "    \"\"\"Removes all files and subdirectories in a given directory.\n",
        "\n",
        "    Args:\n",
        "        directory (str): Path to the directory that will be cleared.\n",
        "    \"\"\"\n",
        "    shutil.rmtree(directory)\n",
        "    os.mkdir(directory)\n",
        "\n",
        "\n",
        "def zip_directory(directory: str,\n",
        "                  output_file_path: str,\n",
        "                  archive_format: str = 'zip'):\n",
        "    \"\"\"Archives a directory with all its files and subdirectories\n",
        "\n",
        "    Args:\n",
        "        directory (str): Directory that will be archived.\n",
        "        output_file_path (str): Path where the checkpoint will be saved.\n",
        "        archive_format (str, optional): Type of archival. Defaults to 'zip'.\n",
        "    \"\"\"\n",
        "    # Extract the filepath without the file extension,\n",
        "    # (shutil automatically adds the extension)\n",
        "    file_path = os.path.splitext(output_file_path)[0]\n",
        "    shutil.make_archive(file_path, archive_format, directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPGwQnplVTNn"
      },
      "source": [
        "## Helper functions for moving files/directories on GCS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZTCvqw8cLctj"
      },
      "source": [
        "Helper functions move a file or directory on GCS so it can be new file or directory can take its place without losing the previous one. This is useful to save the latest version of a file or directory in a fixed location."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eNBiXJoVTNo"
      },
      "outputs": [],
      "source": [
        "def move_gcs_file(old_file_path: str,\n",
        "                  new_file_path: str,\n",
        "                  storage.bucket.Bucket):\n",
        "    \"\"\"Move a file on gcs to a new location.\n",
        "\n",
        "    Args:\n",
        "        old_file_path (str): Path to the original file location.\n",
        "        new_file_path (str): Path to the new location of the file.\n",
        "        bucket (bucket): GCS bucket that contains the file.\n",
        "    \"\"\"\n",
        "    blob = bucket.blob(old_file_path)\n",
        "    if blob.exists():\n",
        "        bucket.rename_blob(blob, new_file_path)\n",
        "\n",
        "\n",
        "def move_gcs_directory(old_directory_path: str,\n",
        "                       new_directory_path: str,\n",
        "                       storage.bucket.Bucket):\n",
        "    \"\"\"Move a directory on gcs to a new location\n",
        "\n",
        "    Args:\n",
        "        old_directory_path (str): Path to the original directory location.\n",
        "        new_directory_path (str): Path to the new location of the directory.\n",
        "        bucket (bucket): GCS bucket that contains the directory.\n",
        "    \"\"\"\n",
        "    blobs = bucket.list_blobs(prefix=old_directory_path)\n",
        "    for blob in blobs:\n",
        "        file_path = blob.name.replace(old_directory_path, new_directory_path)\n",
        "        bucket.rename_blob(blob, file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SmfNUArVTNo"
      },
      "source": [
        "## Helper functions to download files/directories from GCS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WH1p5BynLctl"
      },
      "source": [
        "Helper functions to download files, directories and archived files from GCS to the local file system, these can be used to restore a checkpoint from GCS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3Ia72y_VTNp"
      },
      "outputs": [],
      "source": [
        "def download_gcs_file(gcs_file_path: str,\n",
        "                      local_file_path: str,\n",
        "                      bucket: storage.bucket.Bucket):\n",
        "    \"\"\"Download a file from gcs to a given location.\n",
        "\n",
        "    Args:\n",
        "        gcs_file_path (str): Path to the file on GCS.\n",
        "        local_file_path (str): Local path to save the file.\n",
        "        bucket (bucket): GCS bucket where the file is stored.\n",
        "    \"\"\"\n",
        "    blob = bucket.blob(gcs_file_path)\n",
        "    blob.download_to_filename(local_file_path)\n",
        "\n",
        "\n",
        "def download_gcs_directory(gcs_directory_path: str,\n",
        "                           local_directory_path: str,\n",
        "                           bucket: storage.bucket.Bucket):\n",
        "    \"\"\"Download a directory with all its subdirectories from GCS.\n",
        "    (The directory structure is recreated locally)\n",
        "\n",
        "    Args:\n",
        "        gcs_directory_path (str): Path to the directory on GCS.\n",
        "        local_directory_path (str): Local path where the\n",
        "        directory will be stored.\n",
        "        bucket (bucket): GCS bucket where the directory is stored.\n",
        "    \"\"\"\n",
        "    blobs = bucket.list_blobs(prefix=gcs_directory_path)\n",
        "\n",
        "    for blob in blobs:\n",
        "        local_file_path = blob.name.replace(\n",
        "            gcs_directory_path,\n",
        "            local_directory_path\n",
        "        )\n",
        "        # Recreate the directory structure if necessary.\n",
        "        path = Path(local_file_path).parents[0]\n",
        "        path.mkdir(parents=True, exist_ok=True)\n",
        "        blob.download_to_filename(local_file_path)\n",
        "\n",
        "\n",
        "def download_gcs_archive(gcs_file_path: str,\n",
        "                         local_file_path: str,\n",
        "                         unpack_directory: str,\n",
        "                         bucket: storage.bucket.Bucket):\n",
        "    \"\"\"Downloads an archived file from GCS and unpacks it.\n",
        "\n",
        "    Args:\n",
        "        gcs_file_path (str): Path to the archived file on GCS.\n",
        "        local_file_path (str): Local path to save the archived file.\n",
        "        unpack_directory (str): Local directory where the\n",
        "        archived file will be unpacked.\n",
        "        bucket (bucket): GCS bucket where the archive is stored.\n",
        "    \"\"\"\n",
        "    download_gcs_file(gcs_file_path, local_file_path, bucket)\n",
        "    shutil.unpack_archive(local_file_path, unpack_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPKFgeWdVTNq"
      },
      "source": [
        "## Helper functions to send files/directories to GCS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jIPRIcueLctm"
      },
      "source": [
        "Helper functions to store local files, directories and archived files on GCS, these can be used to backup a checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmuGIDQqVTNr"
      },
      "outputs": [],
      "source": [
        "def write_file_to_gcs(local_file_path: str,\n",
        "                      gcs_file_path: str,\n",
        "                      bucket: storage.bucket.Bucket):\n",
        "    \"\"\"Writes a single file to gcs. The effect of uploading to\n",
        "    an existing blob depends on the “versioning” and “lifecycle”\n",
        "    policies defined on the blob’s bucket. In the absence of\n",
        "    those policies, upload will overwrite any existing contents.\n",
        "    (https://googleapis.dev/python/storage/latest/blobs.html)\n",
        "\n",
        "    Args:\n",
        "        local_file_path (str): Path to the local file that will be sent to GCS.\n",
        "        gcs_file_path (str): Location on gcs where the file will be stored.\n",
        "        bucket (bucket): GCS bucket where the file will be stored.\n",
        "    \"\"\"\n",
        "    blob = bucket.blob(gcs_file_path)\n",
        "    blob.upload_from_filename(Path(local_file_path).absolute())\n",
        "\n",
        "\n",
        "def write_directory_to_gcs(local_directory_path: str,\n",
        "                           gcs_directory_path: str,\n",
        "                           bucket: storage.bucket.Bucket,\n",
        "                           recursive: bool = True):\n",
        "    \"\"\"Write a directory (optionally all its subdirectories) to gcs.\n",
        "    The effect of uploading to an existing blob depends on the\n",
        "    “versioning” and “lifecycle” policies defined on the blob’s bucket. In\n",
        "    the absence of those policies, upload will overwrite any existing contents.\n",
        "    (https://googleapis.dev/python/storage/latest/blobs.html)\n",
        "\n",
        "    Args:\n",
        "        local_directory_path (str): Local path to the directory\n",
        "        that will be sent to GCS.\n",
        "        gcs_directory_path (str): Path on the GCS bucket where\n",
        "        the directory will be stored.\n",
        "        bucket (bucket): GCS bucket where the directory will be stored.\n",
        "        recursive (bool, optional): Recursively search\n",
        "        subdirecties. Defaults to True.\n",
        "    \"\"\"\n",
        "    for local_file_path in list_files(local_directory_path, recursive):\n",
        "        gcs_file_path = local_file_path.replace(\n",
        "            local_directory_path,\n",
        "            gcs_directory_path\n",
        "        )\n",
        "        blob = bucket.blob(gcs_file_path)\n",
        "        blob.upload_from_filename(local_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBy5dcZAmI8Y"
      },
      "source": [
        "## Tensorflow example functions to load a checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOa2iEwLf095"
      },
      "outputs": [],
      "source": [
        "def load_tf_checkpoint_zip(gcs_latest_checkpoint: str,\n",
        "                           arhived_checkpoint: str,\n",
        "                           local_checkpoint: str,\n",
        "                           storage.bucket.Bucket):\n",
        "    \"\"\"Load an archived checkpoint from GCS.\n",
        "\n",
        "    Args:\n",
        "        gcs_latest_checkpoint (str): Path to the archived checkpoint on GCS.\n",
        "        arhived_checkpoint (str): Local path to save the archived checkpoint.\n",
        "        local_checkpoint (str): Local path to the unpacked checkpoint.\n",
        "        bucket (bucket): GCS bucket that contains the checkpoint.\n",
        "\n",
        "    Returns:\n",
        "        tf.keras.Model: Loaded model, ready to to continue training.\n",
        "    \"\"\"\n",
        "    download_gcs_archive(\n",
        "        gcs_latest_checkpoint, arhived_checkpoint,\n",
        "        local_checkpoint, bucket)\n",
        "    model = tf.keras.models.load_model(local_checkpoint)\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_tf_checkpoint_directory(gcs_latest_checkpoint: str,\n",
        "                                 local_checkpoint: str,\n",
        "                                 storage.bucket.Bucket):\n",
        "    \"\"\"Load checkpoint saved as directory from GCS\n",
        "\n",
        "    Args:\n",
        "        gcs_latest_checkpoint (str): Path to the checkpoint directory on GCS.\n",
        "        local_checkpoint (str): Path where the checkpoint will be saved\n",
        "        locally.\n",
        "        bucket (bucket): GCS bucket that contains the checkpoint.\n",
        "\n",
        "    Returns:\n",
        "        tf.keras.Model: Loaded model, ready to to continue training.\n",
        "    \"\"\"\n",
        "    download_gcs_directory(\n",
        "        gcs_latest_checkpoint,\n",
        "        gcs_latest_checkpoint,\n",
        "        bucket)\n",
        "    model = tf.keras.models.load_model(local_checkpoint)\n",
        "    return model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ov1x2AfLVTNs"
      },
      "source": [
        "# Training example"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JqljvjIDVTNs"
      },
      "source": [
        "We will train a simple MLP classifier using the MNIST dataset. We will start from a given checkpoint on GCS and will continue training. Each epoch a checkpoint is made and sent to GCS. You can use this example as guideline for the structure of the boilerplate."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cJC4f2CuVTNs"
      },
      "source": [
        "Load the dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ty39fyepVTNs"
      },
      "outputs": [],
      "source": [
        "mnist_dataset = tf.keras.datasets.mnist\n",
        "(x_train, y_train), (x_test, y_test) = mnist_dataset.load_data()\n",
        "\n",
        "x_train_normalized = x_train / 255\n",
        "x_test_normalized = x_test / 255"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UzQlD2pPVTNt"
      },
      "source": [
        "Create a model and optimizer."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rj0Q4fYyVTNt"
      },
      "outputs": [],
      "source": [
        "model = tf.keras.models.Sequential()\n",
        "\n",
        "# input layer\n",
        "model.add(tf.keras.layers.Flatten(input_shape=x_train_normalized.shape[1:]))\n",
        "model.add(tf.keras.layers.Dense(\n",
        "    units=128,\n",
        "    activation=tf.keras.activations.relu,\n",
        "    kernel_regularizer=tf.keras.regularizers.l2(0.002)\n",
        "))\n",
        "\n",
        "# hidden layers\n",
        "model.add(tf.keras.layers.Dense(\n",
        "    units=128,\n",
        "    activation=tf.keras.activations.relu,\n",
        "    kernel_regularizer=tf.keras.regularizers.l2(0.002)\n",
        "))\n",
        "\n",
        "# output layers\n",
        "model.add(tf.keras.layers.Dense(\n",
        "    units=10,\n",
        "    activation=tf.keras.activations.softmax\n",
        "))\n",
        "\n",
        "adam_optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "\n",
        "model.compile(\n",
        "    optimizer=adam_optimizer,\n",
        "    loss=tf.keras.losses.sparse_categorical_crossentropy,\n",
        "    metrics=['accuracy']\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_sReauUZVTNu"
      },
      "source": [
        "Train the model and save intermediate steps to gcs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "StW8sIMbV5KH"
      },
      "outputs": [],
      "source": [
        "storage_client = storage.Client(project=\"centered-rope-339915\")\n",
        "bucket = storage_client.get_bucket(\"pvmb-training-checkpoints\")\n",
        "local_directory = \"checkpoint-buffer\"\n",
        "gcs_directory = \"tf-mnist-checkpoints\"\n",
        "checkpoint_name = \"latest-checkpoint.zip\"\n",
        "local_checkpoint = checkpoint_name\n",
        "gcs_latest_checkpoint = f\"{gcs_directory}/{checkpoint_name}\"\n",
        "\n",
        "#os.mkdir(local_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mYUnEc8wkKMW"
      },
      "source": [
        "Load a checkpoint from GCS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "og-5CtblkJYf"
      },
      "outputs": [],
      "source": [
        "download_gcs_directory(f\"{gcs_directory}/checkpoint-2\", local_directory, bucket)\n",
        "model = tf.keras.models.load_model(local_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rKvKW8IXVTNu"
      },
      "outputs": [],
      "source": [
        "epochs = 5\n",
        "for epoch in range(epochs):\n",
        "    training_history = model.fit(\n",
        "        x_train_normalized,\n",
        "        y_train,\n",
        "        epochs=1,\n",
        "        validation_data=(x_test_normalized, y_test),\n",
        "    )\n",
        "    # save the model\n",
        "    \"\"\"\n",
        "    save_tf_checkpoint_zip(model,\n",
        "                           local_directory,\n",
        "                           gcs_latest_checkpoint,\n",
        "                           f\"{gcs_directory}/checkpoint-{epoch}.zip\",\n",
        "                           local_checkpoint,\n",
        "                           bucket\n",
        "    )\n",
        "    \"\"\"\n",
        "    save_tf_checkpoint_directory(model,\n",
        "                                 local_directory,\n",
        "                                 gcs_latest_checkpoint,\n",
        "                                 f\"{gcs_directory}/checkpoint-{epoch}\",\n",
        "                                 bucket\n",
        "    )\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "name": "Preemptible Machines - Tensorflow Example",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}