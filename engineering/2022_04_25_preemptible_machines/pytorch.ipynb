{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BipCz3XbVTNe"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import shutil\n",
        "import torch\n",
        "import torch.nn.functional as F\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "from pathlib import Path\n",
        "\n",
        "from torch import nn, optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.datasets import MNIST\n",
        "\n",
        "from google.cloud import storage"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q6_30psqzxqz"
      },
      "outputs": [],
      "source": [
        "# When using Colab, you need to allow it to access your resources on GCP.\n",
        "try:\n",
        "    from google.colab import auth\n",
        "    auth.authenticate_user()\n",
        "except:\n",
        "    pass\n",
        "\n",
        "# Fill in the project-id of your GCP project and the name of \n",
        "# the GCS bucket where you want to store the checkpoints\n",
        "PROJECT = \"my_gcp_project\"\n",
        "BUCKET = \"my_gcp_bucket\""
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0oLIAQ7_VTNj"
      },
      "source": [
        "# Checkpoint helper functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jvy-LB1OVTNk"
      },
      "source": [
        "This section contains the helper functions for storing and loading Tensorflow/Keras checkpoints on Google Cloud Storage. You can use these functions as a starting point and modify them for your use case.\n",
        "\n",
        "The only dependency for this boilerplate is the Python Client for Google Cloud Storage, you can find the docs here:\n",
        "\n",
        "https://googleapis.dev/python/storage/latest/index.html"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCOVNC6Apxd6"
      },
      "source": [
        "## Helper functions for local file system manipulation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TeWWCpq3zQa8"
      },
      "source": [
        "Helper functions for local file system manipulation. Tensorflow does not support to save a checkpoint in an archived format such as `zip` or `tar`. Archiving checkpoints before sending them to GCS can help reducing necessary storage and make the file structure on gcs simpler. It is not necessary to archive checkpoints."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "82lkhbqOVTNl"
      },
      "outputs": [],
      "source": [
        "def list_files(directory: str, recursive: bool = True):\n",
        "    \"\"\"Create a generator that lists all files in a directory.\n",
        "    Optionally also recusrively all list files in subdirectories.\n",
        "\n",
        "    Args:\n",
        "        directory (str): List files in this directory.\n",
        "        recursive (bool, optional): Option to recusrively list files\n",
        "        in subdirectories. Defaults to True.\n",
        "\n",
        "    Returns:\n",
        "        Generator[str]: generator of filepaths in string format.\n",
        "    \"\"\"\n",
        "    if recursive:\n",
        "        filepaths = Path(directory).rglob('*')\n",
        "    else:\n",
        "        filepaths = Path(directory).glob('*')\n",
        "\n",
        "    return (str(child) for child in filepaths if child.is_file())\n",
        "\n",
        "\n",
        "def clear_directory(directory: str):\n",
        "    \"\"\"Removes all files and subdirectories in a given directory.\n",
        "\n",
        "    Args:\n",
        "        directory (str): Path to the directory that will be cleared.\n",
        "    \"\"\"\n",
        "    shutil.rmtree(directory)\n",
        "    os.mkdir(directory)\n",
        "\n",
        "\n",
        "def zip_directory(directory: str,\n",
        "                  output_file_path: str,\n",
        "                  archive_format: str = 'zip'):\n",
        "    \"\"\"Archives a directory with all its files and subdirectories\n",
        "\n",
        "    Args:\n",
        "        directory (str): Directory that will be archived.\n",
        "        output_file_path (str): Path where the checkpoint will be saved.\n",
        "        archive_format (str, optional): Type of archival. Defaults to 'zip'.\n",
        "    \"\"\"\n",
        "    # Extract the filepath without the file extension,\n",
        "    # (shutil automatically adds the extension)\n",
        "    file_path = os.path.splitext(output_file_path)[0]\n",
        "    shutil.make_archive(file_path, archive_format, directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MPGwQnplVTNn"
      },
      "source": [
        "## Helper functions for moving files/directories on GCS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GU9z8TNszQa_"
      },
      "source": [
        "Helper functions move a file or directory on GCS so it can be new file or directory can take its place without losing the previous one. This is useful to save the latest version of a file or directory in a fixed location."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_eNBiXJoVTNo"
      },
      "outputs": [],
      "source": [
        "def move_gcs_file(old_file_path: str, new_file_path: str, bucket):\n",
        "    \"\"\"Move a file on gcs to a new location.\n",
        "\n",
        "    Args:\n",
        "        old_file_path (str): Path to the original file location.\n",
        "        new_file_path (str): Path to the new location of the file.\n",
        "        bucket (bucket): GCS bucket that contains the file.\n",
        "    \"\"\"\n",
        "    blob = bucket.blob(old_file_path)\n",
        "    if blob.exists():\n",
        "        bucket.rename_blob(blob, new_file_path)\n",
        "\n",
        "\n",
        "def move_gcs_directory(old_directory_path: str,\n",
        "                       new_directory_path: str,\n",
        "                       bucket):\n",
        "    \"\"\"Move a directory on gcs to a new location\n",
        "\n",
        "    Args:\n",
        "        old_directory_path (str): Path to the original directory location.\n",
        "        new_directory_path (str): Path to the new location of the directory.\n",
        "        bucket (bucket): GCS bucket that contains the directory.\n",
        "    \"\"\"\n",
        "    blobs = bucket.list_blobs(prefix=old_directory_path)\n",
        "    for blob in blobs:\n",
        "        file_path = blob.name.replace(old_directory_path, new_directory_path)\n",
        "        bucket.rename_blob(blob, file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0SmfNUArVTNo"
      },
      "source": [
        "## Helper functions to download files/directories from GCS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "oZPcHSDUzQbB"
      },
      "source": [
        "Helper functions to download files, directories and archived files from GCS to the local file system, these can be used to restore a checkpoint from GCS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d3Ia72y_VTNp"
      },
      "outputs": [],
      "source": [
        "def download_gcs_file(gcs_file_path: str,\n",
        "                      local_file_path: str,\n",
        "                      bucket: storage.bucket.Bucket):\n",
        "    \"\"\"Download a file from gcs to a given location.\n",
        "\n",
        "    Args:\n",
        "        gcs_file_path (str): Path to the file on GCS.\n",
        "        local_file_path (str): Local path to save the file.\n",
        "        bucket (bucket): GCS bucket where the file is stored.\n",
        "    \"\"\"\n",
        "    blob = bucket.blob(gcs_file_path)\n",
        "    blob.download_to_filename(local_file_path)\n",
        "\n",
        "\n",
        "def download_gcs_directory(gcs_directory_path: str,\n",
        "                           local_directory_path: str,\n",
        "                           bucket: storage.bucket.Bucket):\n",
        "    \"\"\"Download a directory with all its subdirectories from GCS.\n",
        "    (The directory structure is recreated locally)\n",
        "\n",
        "    Args:\n",
        "        gcs_directory_path (str): Path to the directory on GCS.\n",
        "        local_directory_path (str): Local path where the\n",
        "        directory will be stored.\n",
        "        bucket (bucket): GCS bucket where the directory is stored.\n",
        "    \"\"\"\n",
        "    blobs = bucket.list_blobs(prefix=gcs_directory_path)\n",
        "\n",
        "    for blob in blobs:\n",
        "        local_file_path = blob.name.replace(\n",
        "            gcs_directory_path,\n",
        "            local_directory_path\n",
        "        )\n",
        "        # Recreate the directory structure if necessary.\n",
        "        path = Path(local_file_path).parents[0]\n",
        "        path.mkdir(parents=True, exist_ok=True)\n",
        "        blob.download_to_filename(local_file_path)\n",
        "\n",
        "\n",
        "def download_gcs_archive(gcs_file_path: str,\n",
        "                         local_file_path: str,\n",
        "                         unpack_directory: str,\n",
        "                         bucket: storage.bucket.Bucket):\n",
        "    \"\"\"Downloads an archived file from GCS and unpacks it.\n",
        "\n",
        "    Args:\n",
        "        gcs_file_path (str): Path to the archived file on GCS.\n",
        "        local_file_path (str): Local path to save the archived file.\n",
        "        unpack_directory (str): Local directory where the\n",
        "        archived file will be unpacked.\n",
        "        bucket (bucket): GCS bucket where the archive is stored.\n",
        "    \"\"\"\n",
        "    download_gcs_file(gcs_file_path, local_file_path, bucket)\n",
        "    shutil.unpack_archive(local_file_path, unpack_directory)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TPKFgeWdVTNq"
      },
      "source": [
        "## Helper functions to send files/directories to GCS"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06PNh2pCzQbC"
      },
      "source": [
        "Helper functions to store local files, directories and archived files on GCS, these can be used to backup a checkpoint."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SmuGIDQqVTNr"
      },
      "outputs": [],
      "source": [
        "def write_file_to_gcs(local_file_path: str,\n",
        "                      gcs_file_path: str,\n",
        "                      bucket: storage.bucket.Bucket):\n",
        "    \"\"\"Writes a single file to gcs. The effect of uploading to\n",
        "    an existing blob depends on the “versioning” and “lifecycle”\n",
        "    policies defined on the blob’s bucket. In the absence of\n",
        "    those policies, upload will overwrite any existing contents.\n",
        "    (https://googleapis.dev/python/storage/latest/blobs.html)\n",
        "\n",
        "    Args:\n",
        "        local_file_path (str): Path to the local file that will be sent to GCS.\n",
        "        gcs_file_path (str): Location on gcs where the file will be stored.\n",
        "        bucket (bucket): GCS bucket where the file will be stored.\n",
        "    \"\"\"\n",
        "    blob = bucket.blob(gcs_file_path)\n",
        "    blob.upload_from_filename(Path(local_file_path).absolute())\n",
        "\n",
        "\n",
        "def write_directory_to_gcs(local_directory_path: str,\n",
        "                           gcs_directory_path: str,\n",
        "                           bucket: storage.bucket.Bucket,\n",
        "                           recursive: bool = True):\n",
        "    \"\"\"Write a directory (optionally all its subdirectories) to gcs.\n",
        "    The effect of uploading to an existing blob depends on the\n",
        "    “versioning” and “lifecycle” policies defined on the blob’s bucket. In\n",
        "    the absence of those policies, upload will overwrite any existing contents.\n",
        "    (https://googleapis.dev/python/storage/latest/blobs.html)\n",
        "\n",
        "    Args:\n",
        "        local_directory_path (str): Local path to the directory\n",
        "        that will be sent to GCS.\n",
        "        gcs_directory_path (str): Path on the GCS bucket where\n",
        "        the directory will be stored.\n",
        "        bucket (bucket): GCS bucket where the directory will be stored.\n",
        "        recursive (bool, optional): Recursively search\n",
        "        subdirecties. Defaults to True.\n",
        "    \"\"\"\n",
        "    for local_file_path in list_files(local_directory_path, recursive):\n",
        "        gcs_file_path = local_file_path.replace(\n",
        "            local_directory_path,\n",
        "            gcs_directory_path\n",
        "        )\n",
        "        blob = bucket.blob(gcs_file_path)\n",
        "        blob.upload_from_filename(local_file_path)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBy5dcZAmI8Y"
      },
      "source": [
        "### Pytorch example function to save a checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4lrn93sHVTNr"
      },
      "outputs": [],
      "source": [
        "def save_pt_checkpoint(model_state: dict,\n",
        "                       local_directory: str,\n",
        "                       local_checkpoint: str,\n",
        "                       gcs_latest_checkpoint: str,\n",
        "                       gcs_checkpoint_store: str,\n",
        "                       bucket: storage.bucket.Bucket):\n",
        "    \"\"\"Send a checkpoint in archived form to GCS,\n",
        "    moves the current latest checkpoint to a different,\n",
        "    specified location.\n",
        "\n",
        "    Args:\n",
        "        model_state (dict): Dictionary that contains all\n",
        "        checkpoint data that needs to be saved.\n",
        "        local_directory (str): Local path to the directory\n",
        "        where the checkpoint will be saved.\n",
        "        local_checkpoint (str): Local path where\n",
        "        the checkpoint will be saved.\n",
        "        gcs_latest_checkpoint (str): Path where the latest\n",
        "        checkpoint is saved on GCS.\n",
        "        gcs_checkpoint_store (str): Path where the current\n",
        "        last checkpoint on GCS will be moved to.\n",
        "        bucket (bucket): GCS bucket where the checkpoint will be stored.\n",
        "    \"\"\"\n",
        "    # clear the directory where the checkpoint is saved locally\n",
        "    clear_directory(local_directory)\n",
        "    move_gcs_file(gcs_latest_checkpoint, gcs_checkpoint_store, bucket)\n",
        "    torch.save(model_state, local_checkpoint)\n",
        "    write_file_to_gcs(local_checkpoint, gcs_latest_checkpoint, bucket)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uRQeYzd6zQbD"
      },
      "source": [
        "### Pytorch example function to load a checkpoint"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cOa2iEwLf095"
      },
      "outputs": [],
      "source": [
        "def load_pt_checkpoint(gcs_checkpoint: str,\n",
        "                       local_checkpoint: str,\n",
        "                       bucket: storage.bucket.Bucket):\n",
        "    \"\"\"Load an archived checkpoint from GCS\n",
        "\n",
        "    Args:\n",
        "        gcs_checkpoint (str): Path to the checkpoint on GCS.\n",
        "        local_checkpoint (str): Local path to the checkpoint.\n",
        "        bucket (bucket): GCS bucket that contains the checkpoint\n",
        "    \"\"\"\n",
        "    download_gcs_file(gcs_checkpoint, local_checkpoint, bucket)\n",
        "    checkpoint = torch.load(local_checkpoint)\n",
        "    return checkpoint"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ov1x2AfLVTNs"
      },
      "source": [
        "# Training example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BQCo0fa9zQbE"
      },
      "outputs": [],
      "source": [
        "# flatten 28*28 images to a 784 vector for each image\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),  # convert to tensor\n",
        "    transforms.Lambda(lambda x: x.view(-1))  # flatten into vector\n",
        "])\n",
        "\n",
        "trainset = MNIST(\".\", train=True, download=True, transform=transform)\n",
        "testset = MNIST(\".\", train=False, download=True, transform=transform)\n",
        "\n",
        "# create data loaders\n",
        "trainloader = DataLoader(trainset, batch_size=128, shuffle=True)\n",
        "testloader = DataLoader(testset, batch_size=128, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mWfQkh26zQbE"
      },
      "outputs": [],
      "source": [
        "class MLP(nn.Module):\n",
        "    def __init__(self, input_size, hidden_size, num_classes):\n",
        "        super(MLP, self).__init__()\n",
        "        self.fc1 = nn.Linear(input_size, hidden_size) \n",
        "        self.fc2 = nn.Linear(hidden_size, num_classes)  \n",
        "    \n",
        "    def forward(self, x):\n",
        "        out = self.fc1(x)\n",
        "        out = F.relu(out)\n",
        "        out = self.fc2(out)\n",
        "        if not self.training:\n",
        "            out = F.softmax(out, dim=1)\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mMBH_UNnzQbE"
      },
      "outputs": [],
      "source": [
        "storage_client = storage.Client(project=PROJECT)\n",
        "bucket = storage_client.get_bucket(BUCKET)\n",
        "local_directory = \"checkpoint-buffer\"\n",
        "gcs_directory = \"pt-mnist-checkpoints\"\n",
        "checkpoint_name = \"latest-checkpoint.pt\"\n",
        "local_checkpoint = f\"{local_directory}/{checkpoint_name}\"\n",
        "gcs_latest_checkpoint = f\"{gcs_directory}/{checkpoint_name}\"\n",
        "\n",
        "#os.mkdir(local_directory)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GoYhmnUgzQbE"
      },
      "outputs": [],
      "source": [
        "model = MLP(784, 784, 10)\n",
        "\n",
        "# define the loss function and the optimiser\n",
        "loss_function = nn.CrossEntropyLoss()\n",
        "optimiser = optim.Adam(model.parameters())\n",
        "\n",
        "# Load the checkpoint from GCS.\n",
        "checkpoint = load_pt_checkpoint(\n",
        "    gcs_latest_checkpoint,\n",
        "    local_checkpoint, bucket\n",
        ")\n",
        "# Read the checkpoint values and prepare for training.\n",
        "model.load_state_dict(checkpoint['model_state_dict'])\n",
        "optimiser.load_state_dict(checkpoint['optimiser_state_dict'])\n",
        "start_epoch = checkpoint['epoch']\n",
        "\n",
        "\n",
        "# the epoch loop\n",
        "for epoch in range(start_epoch, 5):\n",
        "    \n",
        "    running_loss = 0.0\n",
        "\n",
        "    for data in trainloader:\n",
        "        # get the inputs\n",
        "        inputs, labels = data\n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimiser.zero_grad()\n",
        "\n",
        "        # forward + loss + backward + optimise (update weights)\n",
        "        outputs = model(inputs)\n",
        "        loss = loss_function(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimiser.step()\n",
        "\n",
        "        # keep track of the loss this epoch\n",
        "        running_loss += loss.item()\n",
        "\n",
        "    # Data to save in the checkpoint.\n",
        "    checkpoint_data = {\n",
        "        'epoch': epoch,\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'optimiser_state_dict': optimiser.state_dict(),\n",
        "        'loss': running_loss,\n",
        "    }\n",
        "\n",
        "    # Save the model to GCS.\n",
        "    save_pt_checkpoint(\n",
        "        checkpoint_data,\n",
        "        local_directory,\n",
        "        local_checkpoint,\n",
        "        gcs_latest_checkpoint,\n",
        "        f\"{gcs_directory}/checkpoint-{epoch}.pt\",\n",
        "        bucket\n",
        "    )"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "Preemptible Machines - Pytorch Example",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3 (ipykernel)",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
